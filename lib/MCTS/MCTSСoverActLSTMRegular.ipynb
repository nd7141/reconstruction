{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from pandas import DataFrame\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from utils_mcts import *\n",
    "from MCTS_Act_LSTM import MCTS\n",
    "from problem_mcts import GraphProblem, generate_erdos_renyi_problems, generate_regular_problems\n",
    "from network_mcts import AgentActLSTM\n",
    "import time\n",
    "import nn_utils\n",
    "from collections import defaultdict as ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "NUM_PROBLEMS = 15\n",
    "NUM_EPISODES = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_MCSIMS = 10\n",
    "NUM_UPDATES = 5\n",
    "NUM_VERTICES = 15\n",
    "DEGREE = 6\n",
    "CPUCT = 10\n",
    "THRESHOLD = 0.75\n",
    "PATHS_BUFFER_CAPACITY = 100\n",
    "REPLAY_BUFFER_CAPACITY = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate regular train graphs (n=15, d=6)\n",
    "problem_maker = generate_erdos_renyi_problems(num_vertices=NUM_VERTICES, edge_prob=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent\n",
    "agent = AgentActLSTM(hid_size=256, gcn_size=256, vertex_emb_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize buffers\n",
    "path_buffer = PathsBuffer(capacity=PATHS_BUFFER_CAPACITY, threshold=THRESHOLD)\n",
    "train_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss stats\n",
    "pi_losses = []\n",
    "v_losses = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [next(problem_maker) for i in range(NUM_PROBLEMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems[0].edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for k in trange(len(problems)):\n",
    "    \n",
    "    problem = problems[k]\n",
    "    \n",
    "    edges = problem.get_edges()\n",
    "\n",
    "    for vertex in problem.get_actions():\n",
    "\n",
    "        path_buffer.flush()\n",
    "    \n",
    "        PATH_LENGTH = 2*problem.num_edges + 1        \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "        for episode in range(NUM_EPISODES):\n",
    "            \n",
    "            problem.path = [vertex]\n",
    "        \n",
    "            source = problem.get_state()[0]\n",
    "            \n",
    "            states = []\n",
    "            actions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                graph_emb = agent.embed_graph(problem.edges)\n",
    "                \n",
    "            mcts = MCTS(game=problem, nnet=agent, graph_emb=graph_emb,\n",
    "                        numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, edges=edges, path_length=PATH_LENGTH)\n",
    "            \n",
    "            trainExamples = []\n",
    "                \n",
    "            random_walk = [source]\n",
    "            checked = ddict(list)\n",
    "            stack = [source]\n",
    "            visited = {source}\n",
    "            ranks = {0: source} # to attempt to get maximal cover (possible to do without rank, but then no guarantees on maximality)\n",
    "            revranks = {source: 0}\n",
    "            \n",
    "            while len(stack) > 0:\n",
    "                last = stack[-1]\n",
    "                lastrank = revranks[last]\n",
    "                maxrank = max(ranks.keys()) + 1\n",
    "                with torch.no_grad():\n",
    "                    pi = mcts.getActionProb(random_walk, path_buffer)\n",
    "                Nlast = [x for _,x in sorted(zip(pi, edges[random_walk[:][-1]]), reverse=True)]\n",
    "                #print(\"Is valid\", all(i in edges[random_walk[:][-1]] for i in Nlast))\n",
    "                # going in depth\n",
    "                flag = False\n",
    "                for neighbor in Nlast:\n",
    "                    if neighbor not in visited:\n",
    "                        trainExamples.append([random_walk[:], pi, None])\n",
    "                        random_walk.append(neighbor)\n",
    "                        stack.append(neighbor)\n",
    "                        checked[last].append(neighbor)\n",
    "                        visited.add(neighbor)\n",
    "                        ranks[maxrank] = neighbor\n",
    "                        revranks[neighbor] = maxrank\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "                # interconnecting nodes that are already in walk\n",
    "                if not flag:\n",
    "                    for r in range(maxrank-1, lastrank+1, -1):\n",
    "                        node = ranks[r]\n",
    "                        if node not in checked[last] and node in Nlast:\n",
    "                            checked[last].append(node)\n",
    "                            random_walk.extend([node, last])\n",
    "\n",
    "                if not flag:\n",
    "                    stack.pop()\n",
    "                    if len(stack) > 0:\n",
    "                        random_walk.append(stack[-1])\n",
    "                        checked[last].append(stack[-1])\n",
    "                        \n",
    "            print(check_that_cover(random_walk, problem))\n",
    "                               \n",
    "            path_buffer.push(random_walk)\n",
    "            if len(path_buffer) >= 10: \n",
    "                r = path_buffer.rank_path(random_walk)\n",
    "                rewards.append(r)\n",
    "                for x in trainExamples:\n",
    "                    x[-1] = r\n",
    "                train_buffer.push(trainExamples)\n",
    "            \n",
    "            if len(train_buffer) >= BATCH_SIZE:\n",
    "                print(\"Start training!\")\n",
    "                for i in range(NUM_UPDATES):\n",
    "                    batch = train_buffer.sample(BATCH_SIZE)\n",
    "                    paths, pis, vs = zip(*batch)\n",
    "                    graph_emb = agent.embed_graph(problem.edges)\n",
    "                    out_pi, out_v = agent.get_dist(list(paths), graph_emb, edges)\n",
    "\n",
    "                    target_vs = torch.tensor(vs)\n",
    "\n",
    "                    losses_pi = []\n",
    "                    for i, p in enumerate(pis):\n",
    "                        losses_pi.append(torch.sum(torch.tensor(p)*torch.log(out_pi[i])))\n",
    "\n",
    "                    loss_pi = -torch.sum(torch.stack(losses_pi))/len(pis)\n",
    "                    loss_v = torch.sum((target_vs-out_v.view(-1))**2)/target_vs.size()[0]\n",
    "                    total_loss = loss_pi + loss_v\n",
    "\n",
    "                    pi_losses.append(loss_pi.item())\n",
    "                    v_losses.append(loss_v.item())\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    clear_output(True)\n",
    "                    plt.figure(figsize=[15, 6])\n",
    "                    plt.subplot(1,3,1)\n",
    "                    plt.title('Policy loss'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(pi_losses)), pi_losses, alpha=0.1)\n",
    "                    plt.plot(moving_average(pi_losses, span=10, min_periods=10))\n",
    "\n",
    "                    plt.subplot(1,3,2)\n",
    "                    plt.title('Value loss'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(v_losses)), v_losses, alpha=0.1)\n",
    "                    plt.plot(moving_average(v_losses, span=10, min_periods=10))\n",
    "\n",
    "                    plt.subplot(1,3,3)\n",
    "                    plt.title('Mean reward'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(rewards)), rewards, alpha=0.1)\n",
    "                    plt.plot(moving_average(rewards, span=10, min_periods=10))\n",
    "                    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
