{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from pandas import DataFrame\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from utils_mcts import ReplayBuffer, PathsBuffer, get_states_emb, convert_to_walk\n",
    "from MCTS import MCTS\n",
    "from problem_mcts import GraphProblem, generate_erdos_renyi_problems, generate_regular_problems\n",
    "from network_mcts import Agent\n",
    "import time\n",
    "import nn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(P, source, target):\n",
    "    '''Replace last occurrence of source with source-target-source.'''\n",
    "    assert source in P\n",
    "    ix = len(P) - P[::-1].index(source)\n",
    "    return P[:ix] + [target, P[ix - 1]] + P[ix:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covering_walk(graph, source):\n",
    "    P = [0]  # supporting walk\n",
    "    S = [0]  # stack of nodes to check\n",
    "    node2anon = {source: 0}\n",
    "    anon2node = {0: source}\n",
    "    checked = dict()  # nodes that has been checked for edge\n",
    "    degrees = graph.degree()\n",
    "    while len(S) > 0:  # grow supporting walk in DFS manner\n",
    "        curr = S[-1]\n",
    "        x = max(P) + 1  # next node to check\n",
    "\n",
    "        # check if there is a node in the neighborhood that has not been explored yet\n",
    "        Ncurr = list(nx.neighbors(graph, anon2node[curr]))\n",
    "        if random.uniform(0, 1) < 0.99:\n",
    "            random.shuffle(Ncurr)  # option 1: random order\n",
    "        else:\n",
    "            Ncurr = sorted(Ncurr, key=lambda v: degrees[v], reverse=True)  # option 2: top-degree\n",
    "            # Ncurr = sorted(Ncurr, key=lambda v: degrees[v], reverse=False)  # option 3: low-degree\n",
    "        # print(anon2node[curr], Ncurr)\n",
    "        for neighbor in Ncurr:\n",
    "            if neighbor in node2anon:\n",
    "                continue  # already visited\n",
    "            else:\n",
    "                node2anon[neighbor] = x\n",
    "                anon2node[x] = neighbor\n",
    "                S.append(x)\n",
    "                checked.setdefault(curr, set()).add(x)\n",
    "                P = replace(P, curr, x)  # move to it\n",
    "                break\n",
    "        else:\n",
    "            S.pop()  # move back in the stack\n",
    "\n",
    "        for u in range(x-1, curr, -1):  # u is already in the supporting walk\n",
    "            # check if there is connection to already discovered nodes\n",
    "            if u not in checked[curr]:  # see if we already checked this edge\n",
    "                if anon2node[u] in graph[anon2node[curr]]:\n",
    "                    P = replace(P, curr, u)\n",
    "                checked.setdefault(curr, set()).add(u)\n",
    "\n",
    "    cover = [anon2node[v] for v in P]\n",
    "    return cover, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "NUM_PROBLEMS = 50\n",
    "NUM_EPISODES = 50\n",
    "BATCH_SIZE = 32\n",
    "NUM_MCSIMS = 50\n",
    "NUM_UPDATES = 5\n",
    "NUM_VERTICES = 15\n",
    "DEGREE = 6\n",
    "CPUCT = 1.0\n",
    "THRESHOLD = 0.75\n",
    "PATHS_BUFFER_CAPACITY = 1000\n",
    "REPLAY_BUFFER_CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate regular train graphs (n=15, d=6)\n",
    "problem_maker = generate_erdos_renyi_problems(num_vertices=NUM_VERTICES, edge_prob=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent\n",
    "agent = Agent(hid_size=256, gcn_size=256, vertex_emb_size=64, num_vertices=NUM_VERTICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize buffers\n",
    "path_buffer = PathsBuffer(capacity=PATHS_BUFFER_CAPACITY, threshold=THRESHOLD)\n",
    "train_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss stats\n",
    "pi_losses_history = []\n",
    "v_losses_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [next(problem_maker) for i in range(NUM_PROBLEMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'path_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-50551e0021c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             mcts = MCTS(game=problem, nnet=agent, graph_emb=graph_emb,\n\u001b[0;32m---> 24\u001b[0;31m                         numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mrandom_walk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_length' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for k in trange(len(problems)):\n",
    "    \n",
    "    problem = problems[k]\n",
    "\n",
    "    for vertex in problem.get_actions():\n",
    "\n",
    "        path_buffer.flush()\n",
    "    \n",
    "        PATH_LENGTH = 2*problem.num_edges + 1        \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "        for i in range(NUM_EPISODES):\n",
    "            \n",
    "            problem.path = [vertex]\n",
    "        \n",
    "            source = problem.get_state()[0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                graph_emb = agent.embed_graph(problem.edges)\n",
    "                \n",
    "            mcts = MCTS(game=problem, nnet=agent, graph_emb=graph_emb,\n",
    "                        numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n",
    "                \n",
    "            random_walk = [source]\n",
    "            checked = ddict(list)\n",
    "            stack = [source]\n",
    "            visited = {source}\n",
    "            ranks = {0: source} # to attempt to get maximal cover (possible to do without rank, but then no guarantees on maximality)\n",
    "            revranks = {source: 0}\n",
    "            \n",
    "            trainExamples = []\n",
    "            \n",
    "            while len(stack) > 0:\n",
    "                last = stack[-1]\n",
    "                lastrank = revranks[last]\n",
    "                maxrank = max(ranks.keys()) + 1\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pi = mcts.getActionProb(random_walk)\n",
    "                \n",
    "                Nlast = np.argsort(pi)\n",
    "\n",
    "                # going in depth\n",
    "                for neighbor in Nlast:\n",
    "                    if neighbor not in visited: # found new node, then add it to the walk\n",
    "                        trainExamples.append([random_walk, pi, None])\n",
    "                        random_walk.append(neighbor)\n",
    "                        stack.append(neighbor)\n",
    "                        checked[last].append(neighbor)\n",
    "                        visited.add(neighbor)\n",
    "                        ranks[maxrank] = neighbor\n",
    "                        revranks[neighbor] = maxrank\n",
    "                        break\n",
    "                else: # we didn't find any new neighbor and rollback\n",
    "                    stack.pop()\n",
    "                    if len(stack) > 0:\n",
    "                        random_walk.append(stack[-1])\n",
    "                        checked[last].append(stack[-1])\n",
    "\n",
    "                # interconnecting nodes that are already in walk\n",
    "                for r in range(maxrank-1, lastrank+1, -1):\n",
    "                    node = ranks[r]\n",
    "                    if node not in checked[last] and node in Nlast:\n",
    "                        checked[last].append(node)\n",
    "                        random_walk.extend([node, last])\n",
    "        \n",
    "            path_buffer.push(random_walk)\n",
    "            if len(path_buffer) >= 10: \n",
    "                r = path_buffer.rank_path(random_walk)\n",
    "                for x in trainExamples:\n",
    "                    x[-1] = r\n",
    "                train_buffer.push(trainExamples)\n",
    "            \n",
    "        if len(train_buffer) >= BATCH_SIZE:\n",
    "            for i in range(NUM_UPDATES):\n",
    "                batch = train_buffer.sample(BATCH_SIZE)\n",
    "                paths, pis, vs = zip(*batch)\n",
    "                embs = get_states_emb(paths, graph_emb)\n",
    "\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                out_pi, out_v = agent(embs)\n",
    "                loss_pi = -torch.sum(target_pis*out_pi)/target_pis.size()[0]\n",
    "                loss_v = torch.sum((target_vs-out_v.view(-1))**2)/target_vs.size()[0]\n",
    "                total_loss = loss_pi + loss_v\n",
    "\n",
    "                pi_losses_history.append(loss_pi.item())\n",
    "                v_losses_history.append(loss_v.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if iteration % 5 == 0:\n",
    "                    clear_output(True)\n",
    "                    plt.figure(figsize=[12, 6])\n",
    "                    plt.subplot(1,2,1)\n",
    "                    plt.title('Policy error'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(pi_losses_history)), pi_losses_history, alpha=0.1)\n",
    "                    plt.plot(moving_average(pi_losses_history, span=100, min_periods=100))\n",
    "\n",
    "                    plt.subplot(1,2,2)\n",
    "                    plt.title('Value error'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(v_losses_history)), v_losses_history, alpha=0.1)\n",
    "                    plt.plot(moving_average(v_losses_history, span=10, min_periods=10))\n",
    "                    plt.show()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next(problem_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.path = [random.sample(list(p.edges.keys()), 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)\n",
    "path_length = 2*p.num_edges+1\n",
    "mcts = MCTS(game=p, nnet=agent, graph_emb=graph_emb,\n",
    "                    numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n",
    "path = p.get_state()\n",
    "while len(path) != path_length:\n",
    "    with torch.no_grad():\n",
    "        pi = mcts.getActionProb(path)\n",
    "    vertex = np.random.choice(len(pi), p=pi)\n",
    "    path = p.get_next_state(path, vertex)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([1, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(256, hidden_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(256, 10, num_layers=10, batch_first=True)\n"
     ]
    }
   ],
   "source": [
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in x:\n",
    "    l.append(graph_emb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.stack(l).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9286,  0.1171,  0.5416,  ...,  0.6903,  0.7174,  0.1541],\n",
       "         [-1.1723,  0.0236,  1.1856,  ...,  1.2111,  1.4870,  0.0660],\n",
       "         [-0.8155, -0.0119,  0.6863,  ...,  0.7252,  0.9753,  0.2304],\n",
       "         [-0.9070,  0.1316,  0.5729,  ...,  0.4311,  0.5688,  0.2329],\n",
       "         [-1.0753,  0.0830,  0.9038,  ...,  0.7697,  0.9993,  0.1767]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = l.view(len(x), 1 , -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9286,  0.1171,  0.5416,  ...,  0.6903,  0.7174,  0.1541]],\n",
       "\n",
       "        [[-1.1723,  0.0236,  1.1856,  ...,  1.2111,  1.4870,  0.0660]],\n",
       "\n",
       "        [[-0.8155, -0.0119,  0.6863,  ...,  0.7252,  0.9753,  0.2304]],\n",
       "\n",
       "        [[-0.9070,  0.1316,  0.5729,  ...,  0.4311,  0.5688,  0.2329]],\n",
       "\n",
       "        [[-1.0753,  0.0830,  0.9038,  ...,  0.7697,  0.9993,  0.1767]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0192,  1.5607,  0.0075,  0.1521, -1.5922, -0.2616, -0.7007,\n",
       "          -1.3618, -0.1222, -1.4695]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0451,  0.1707, -0.1887, -0.0803, -0.3338,  0.0849,  0.0994,\n",
       "          -0.0092,  0.0209, -0.2359],\n",
       "         [ 0.0701,  0.2317, -0.1775, -0.0161, -0.2372,  0.0775,  0.0865,\n",
       "          -0.0051,  0.0095, -0.3846],\n",
       "         [ 0.1011,  0.2673, -0.2765, -0.0446, -0.2541,  0.1182,  0.1001,\n",
       "          -0.0210,  0.0391, -0.4430],\n",
       "         [ 0.0816,  0.3171, -0.3153, -0.0760, -0.3497,  0.1926,  0.0474,\n",
       "          -0.0311,  0.0793, -0.5933],\n",
       "         [ 0.0661,  0.3829, -0.2475, -0.0384, -0.2970,  0.2038,  0.0891,\n",
       "          -0.0173,  0.0254, -0.7366]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_embs, hidden = lstm(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0695,  0.4406, -0.2393,  0.4299, -0.3749,  0.7223, -0.1267,\n",
       "            0.1923, -0.2109,  0.0517]],\n",
       " \n",
       "         [[-0.1012,  0.2199,  0.1219, -0.0128, -0.2094, -0.0616,  0.0540,\n",
       "            0.1520,  0.0629,  0.0184]],\n",
       " \n",
       "         [[ 0.0740,  0.0130, -0.0857, -0.1324, -0.0572,  0.0302,  0.2230,\n",
       "            0.0934, -0.0153,  0.1301]],\n",
       " \n",
       "         [[ 0.0095, -0.0506,  0.0270, -0.0427,  0.1398,  0.1236,  0.0012,\n",
       "           -0.1071,  0.0447,  0.0012]],\n",
       " \n",
       "         [[ 0.0429,  0.1184,  0.2333, -0.0728, -0.0811,  0.2270,  0.0814,\n",
       "           -0.0912,  0.0040,  0.1718]],\n",
       " \n",
       "         [[ 0.1744,  0.0111, -0.1330, -0.1384, -0.0017,  0.0188,  0.0360,\n",
       "            0.1133, -0.0173, -0.1226]],\n",
       " \n",
       "         [[ 0.0173,  0.0233,  0.0372,  0.0795, -0.2491, -0.0192, -0.1484,\n",
       "           -0.1563,  0.2679, -0.0179]],\n",
       " \n",
       "         [[ 0.0739, -0.0769, -0.1860,  0.0529, -0.1925, -0.0917,  0.0945,\n",
       "           -0.1328,  0.1562,  0.1146]],\n",
       " \n",
       "         [[-0.1203, -0.1043,  0.2104, -0.0267,  0.0551, -0.1584,  0.1248,\n",
       "            0.1935, -0.2197,  0.0075]],\n",
       " \n",
       "         [[ 0.0293,  0.1679, -0.0138, -0.0610,  0.0184,  0.0475,  0.0631,\n",
       "           -0.1595,  0.2108,  0.0741]]], grad_fn=<StackBackward>),\n",
       " tensor([[[ 0.1069,  0.7045, -0.2606,  1.6681, -0.9271,  0.9159, -0.1331,\n",
       "            0.2985, -0.2851,  0.5930]],\n",
       " \n",
       "         [[-0.2573,  0.5152,  0.2877, -0.0216, -0.5066, -0.1295,  0.1275,\n",
       "            0.3213,  0.1486,  0.0433]],\n",
       " \n",
       "         [[ 0.1200,  0.0244, -0.1631, -0.2564, -0.1273,  0.0591,  0.6328,\n",
       "            0.1897, -0.0310,  0.2507]],\n",
       " \n",
       "         [[ 0.0178, -0.0935,  0.0730, -0.0812,  0.3808,  0.3131,  0.0022,\n",
       "           -0.2655,  0.0957,  0.0024]],\n",
       " \n",
       "         [[ 0.0897,  0.2719,  0.4212, -0.1668, -0.2014,  0.5010,  0.1871,\n",
       "           -0.2005,  0.0065,  0.3752]],\n",
       " \n",
       "         [[ 0.5000,  0.0258, -0.2175, -0.2987, -0.0035,  0.0503,  0.0845,\n",
       "            0.2723, -0.0311, -0.2266]],\n",
       " \n",
       "         [[ 0.0337,  0.0454,  0.0840,  0.1445, -0.3941, -0.0471, -0.2452,\n",
       "           -0.2772,  0.5442, -0.0478]],\n",
       " \n",
       "         [[ 0.1388, -0.1381, -0.3086,  0.0856, -0.4366, -0.1881,  0.1624,\n",
       "           -0.2658,  0.2575,  0.3389]],\n",
       " \n",
       "         [[-0.2440, -0.2061,  0.5515, -0.0497,  0.0962, -0.3341,  0.2910,\n",
       "            0.3901, -0.4476,  0.0137]],\n",
       " \n",
       "         [[ 0.0774,  0.5427, -0.0275, -0.0895,  0.0399,  0.0898,  0.1652,\n",
       "           -0.3351,  0.4567,  0.1477]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1622,  0.0380,  0.0270, -0.0840, -0.0621, -0.0490,  0.0329,  0.0838,\n",
       "         0.0326,  0.1116], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[:, -1, :][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.tensor([[2., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1622,  0.0380,  0.0270, -0.0840, -0.0621, -0.0490,  0.0329,  0.0838,\n",
       "          0.0326,  0.1116,  2.0000,  5.0000,  6.0000]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((walk_embs[:, -1, :], f), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only one dimension can be inferred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-367-a484231d13f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: only one dimension can be inferred"
     ]
    }
   ],
   "source": [
    "hn.view(-1, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(walk_embs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1932]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2824,  0.3015, -0.2668,  ...,  0.2538,  0.1729, -0.2005],\n",
       "        [ 0.3229,  0.2920, -0.2846,  ...,  0.2517,  0.1716, -0.1769],\n",
       "        [ 0.3446,  0.2820, -0.2939,  ...,  0.2483,  0.1650, -0.1690],\n",
       "        [ 0.3562,  0.2752, -0.2993,  ...,  0.2455,  0.1587, -0.1662],\n",
       "        [ 0.3624,  0.2713, -0.3025,  ...,  0.2434,  0.1541, -0.1650]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-66887fa826b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 0 with size 5"
     ]
    }
   ],
   "source": [
    "embs[len([3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 3) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 2.0531,  0.2745, -0.0366]]),\n",
       " tensor([[ 0.0588,  0.5562, -1.3053]]),\n",
       " tensor([[0.4513, 0.2132, 1.7349]]),\n",
       " tensor([[-0.0582,  1.3860,  0.5462]]),\n",
       " tensor([[-0.2682, -0.8007, -0.0899]])]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0531,  0.2745, -0.0366]],\n",
       "\n",
       "        [[ 0.0588,  0.5562, -1.3053]],\n",
       "\n",
       "        [[ 0.4513,  0.2132,  1.7349]],\n",
       "\n",
       "        [[-0.0582,  1.3860,  0.5462]],\n",
       "\n",
       "        [[-0.2682, -0.8007, -0.0899]]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {2, 5, 8, 11, 14},\n",
       "             2: {0, 1, 7, 8, 9, 14},\n",
       "             5: {0, 3, 7, 9, 12, 13},\n",
       "             8: {0, 2, 9, 13},\n",
       "             11: {0, 1, 10, 12, 13},\n",
       "             14: {0, 2, 3, 10, 12, 13},\n",
       "             1: {2, 3, 11, 13},\n",
       "             3: {1, 5, 6, 14},\n",
       "             13: {1, 4, 5, 6, 8, 10, 11, 12, 14},\n",
       "             7: {2, 4, 5},\n",
       "             9: {2, 5, 8},\n",
       "             6: {3, 12, 13},\n",
       "             4: {7, 12, 13},\n",
       "             12: {4, 5, 6, 11, 13, 14},\n",
       "             10: {11, 13, 14}})"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [[2, 7, 5, 9], [2, 7, 5, 9, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_emb = get_states_emb(paths, graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_emb = graph_emb[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_emb = next_emb.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-467-bccf75fad53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m q_network_inputs = torch.stack([\n\u001b[0;32m----> 2\u001b[0;31m torch.cat([[get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)] for next_vertex in {2, 5, 8}]\n\u001b[0m\u001b[1;32m      3\u001b[0m             )])\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "q_network_inputs = torch.stack([\n",
    "torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)] for next_vertex in {2, 5, 8}]\n",
    "            )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 512 and 256 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-488-5966dd4e06a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_states_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_vertex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_vertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-488-5966dd4e06a5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_states_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_vertex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_vertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 512 and 256 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "[torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)]) for next_vertex in {2, 5, 8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -1.0753e+00,  8.3016e-02,  9.0379e-01,\n",
      "         -4.5027e-01, -4.9623e-01, -3.3664e-01,  1.6372e-01,  7.8192e-01,\n",
      "         -6.3847e-01,  8.2804e-01, -7.6104e-01, -1.3896e+00, -1.6419e-01,\n",
      "          6.8150e-01, -1.0698e+00,  3.1216e-01, -5.0953e-01,  5.5165e-01,\n",
      "         -1.0343e-01, -3.3589e-02,  1.1074e+00, -9.2738e-01,  5.6787e-01,\n",
      "          9.7487e-01,  4.1884e-02, -2.8618e-01,  3.1022e-01,  1.7354e+00,\n",
      "         -2.1475e-01, -5.0389e-01,  1.1795e+00,  9.8472e-01, -6.9413e-01,\n",
      "         -8.1964e-01,  1.0435e+00,  9.9066e-01,  1.0890e+00,  3.0548e-01,\n",
      "          7.1718e-01,  2.9464e-01, -4.4148e-01,  4.2255e-02, -1.3382e-01,\n",
      "         -1.1807e+00,  8.5676e-01,  1.3887e-01,  1.0270e-01, -5.3805e-01,\n",
      "          8.6735e-02,  1.9163e-01,  1.1998e+00,  9.6794e-01, -5.3834e-01,\n",
      "         -1.0429e+00, -1.0141e-01, -1.2548e-01, -4.8603e-01, -2.8472e-01,\n",
      "          6.8429e-02, -4.5848e-01,  1.0943e+00, -3.3143e-02,  4.7038e-01,\n",
      "          1.7765e-01,  8.0808e-01, -9.2209e-01, -1.4332e-01,  8.2519e-02,\n",
      "         -4.9489e-01,  6.1177e-01,  1.2491e+00, -1.1702e+00,  3.1776e-01,\n",
      "         -1.0395e+00,  4.4867e-01, -9.2202e-02,  5.1838e-01,  1.6764e-01,\n",
      "          1.8922e-01, -8.0567e-01, -4.6363e-01, -1.3145e+00,  7.8315e-01,\n",
      "         -3.3179e-01,  7.9623e-01, -6.0810e-01, -9.0123e-01, -1.3840e-01,\n",
      "          9.1656e-01, -1.2249e+00,  2.8279e-01,  3.8269e-01,  1.3132e+00,\n",
      "          5.3049e-01,  2.9040e-01,  5.5860e-01, -1.4551e-01,  2.1765e-01,\n",
      "          1.5090e-01, -4.5296e-01,  6.2718e-01,  1.2221e+00, -6.4133e-01,\n",
      "          1.4760e+00, -3.4876e-01, -5.5059e-01,  5.1183e-01, -7.2533e-01,\n",
      "          4.9328e-01, -7.2075e-01, -2.4347e-01, -1.4319e-02,  4.9916e-01,\n",
      "         -1.8595e-01,  7.6508e-01, -2.9328e-01,  8.8973e-02,  1.4872e-01,\n",
      "          1.9510e-01,  1.5322e-01,  6.1525e-01,  1.7307e+00, -5.1634e-01,\n",
      "         -2.3653e-01,  1.1662e-01, -6.0022e-01,  4.3567e-01,  3.6696e-01,\n",
      "         -9.3369e-01,  9.2049e-01,  7.9412e-01,  3.8536e-01,  5.2863e-01,\n",
      "         -1.2859e+00, -9.0379e-03, -3.4594e-02, -1.0326e+00, -2.6977e-01,\n",
      "          1.0193e+00, -4.6771e-01,  4.8758e-01,  7.4471e-01,  1.1128e+00,\n",
      "         -8.8620e-01,  2.0674e-01,  3.3264e-01, -7.7463e-01, -3.9493e-01,\n",
      "         -7.0258e-01,  2.3214e-01,  1.6084e+00, -1.6532e-02,  3.2143e-01,\n",
      "          1.7155e-02, -2.3366e-01, -5.0310e-01,  4.2611e-01,  3.2076e-01,\n",
      "          4.3838e-01,  1.4198e+00,  1.0128e+00,  5.4539e-01, -2.0881e-01,\n",
      "          7.8958e-01, -7.0304e-01, -1.0269e+00, -9.4981e-01,  8.2906e-01,\n",
      "          3.4686e-01, -1.0571e+00, -1.7160e-01, -4.9093e-01,  1.4063e+00,\n",
      "          6.8785e-02, -1.8559e-01, -2.6422e-01, -6.0021e-01, -3.8235e-01,\n",
      "         -2.9070e-01, -7.9425e-01, -6.0181e-01,  3.4000e-01,  8.0562e-02,\n",
      "         -2.8659e-01,  3.7100e-01,  5.8875e-01, -1.0452e+00, -1.2472e+00,\n",
      "          9.1564e-02, -2.0688e-01, -1.5108e-01,  8.5732e-01,  7.6452e-01,\n",
      "         -4.2799e-01,  6.6479e-01, -7.8041e-02, -7.8257e-01, -1.2416e-02,\n",
      "         -5.5353e-01,  2.4315e-01, -8.6182e-02,  7.9388e-01, -5.1352e-01,\n",
      "         -1.2207e+00,  9.7136e-02, -7.8171e-01,  3.5285e-01,  6.7947e-01,\n",
      "          5.3724e-01, -1.2459e+00,  1.8495e-01,  7.0043e-01, -3.5441e-02,\n",
      "          5.1074e-01, -1.1365e+00, -3.4992e-01, -1.0458e+00, -2.7660e-01,\n",
      "          1.1339e+00, -3.7067e-01,  5.1654e-01, -2.8271e-01, -8.9785e-01,\n",
      "         -3.5456e-01,  2.9707e-01, -7.4910e-01,  2.5008e-01, -5.8589e-01,\n",
      "          4.4527e-01, -5.1481e-01, -2.7125e-01,  3.2400e-01, -3.5406e-01,\n",
      "         -9.1517e-01, -1.2014e+00, -1.2017e-01,  3.1524e-01, -1.5097e-01,\n",
      "          8.9411e-01, -8.5860e-01, -2.8956e-01, -5.7616e-01, -9.2444e-01,\n",
      "         -4.8267e-01,  1.2769e-01,  1.8004e+00,  3.1816e-01, -2.1505e-01,\n",
      "          4.8580e-01,  8.3232e-01, -8.3170e-03,  1.3702e-01, -1.4425e-01,\n",
      "          7.6971e-01,  9.9928e-01,  1.7671e-01]], grad_fn=<CatBackward>)\n",
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -9.4111e-01,  6.9399e-02,  7.8901e-01,\n",
      "         -9.0577e-01, -7.8137e-01, -3.2307e-01,  1.3018e-01,  1.1046e+00,\n",
      "         -5.6568e-01,  1.0193e+00, -4.5313e-01, -1.1985e+00, -5.1469e-01,\n",
      "          5.2364e-01, -1.2426e+00,  4.5910e-01, -5.6564e-01,  8.7383e-01,\n",
      "          3.9400e-02,  6.3916e-02,  1.2248e+00, -1.2303e+00,  8.1613e-01,\n",
      "          1.0470e+00, -1.8461e-01, -5.0045e-01,  5.9847e-01,  2.0177e+00,\n",
      "         -3.5044e-01, -5.0608e-01,  1.0964e+00,  1.0660e+00, -1.1168e+00,\n",
      "         -9.0968e-01,  1.2758e+00,  1.1305e+00,  1.1310e+00,  7.3110e-01,\n",
      "          8.7305e-01,  3.6757e-01, -5.5817e-01,  2.1573e-01, -1.2241e-01,\n",
      "         -1.4266e+00,  9.8910e-01,  2.4861e-01,  5.1556e-01, -4.8574e-01,\n",
      "          2.8105e-01,  2.1997e-01,  1.6452e+00,  8.5567e-01, -5.2171e-01,\n",
      "         -1.1339e+00, -1.8351e-01,  9.3438e-02, -7.6569e-01, -1.9778e-01,\n",
      "          2.4157e-01, -4.6584e-01,  1.4769e+00,  8.2124e-02,  9.2741e-01,\n",
      "          5.0643e-02,  1.1705e+00, -9.5360e-01, -2.5615e-01, -3.8089e-03,\n",
      "         -3.4076e-01,  6.5082e-01,  1.0418e+00, -1.2011e+00,  3.5959e-01,\n",
      "         -1.0397e+00,  5.0469e-01, -2.8869e-01,  5.6988e-01,  4.1276e-02,\n",
      "          2.7797e-01, -8.5169e-01, -2.8200e-01, -1.1773e+00,  8.1556e-01,\n",
      "         -2.8439e-01,  6.8981e-01, -9.3031e-01, -8.0251e-01, -1.8828e-01,\n",
      "          1.0650e+00, -1.3325e+00,  2.6191e-01,  3.2370e-01,  1.0845e+00,\n",
      "          5.6503e-01,  4.0922e-01,  2.0729e-01, -1.8715e-01,  1.9647e-01,\n",
      "         -1.2909e-01, -7.4647e-01,  9.1441e-01,  1.1838e+00, -3.9645e-01,\n",
      "          1.6914e+00, -3.5210e-01, -3.4453e-01,  5.8520e-01, -4.0602e-01,\n",
      "          7.7554e-01, -1.0866e+00, -5.0612e-02, -9.7199e-02,  5.3288e-01,\n",
      "         -6.1032e-02,  1.0681e+00, -5.9042e-01,  1.7983e-01,  3.0896e-01,\n",
      "          1.5855e-01,  2.0271e-01,  3.5542e-01,  1.9156e+00, -5.0487e-01,\n",
      "          2.1560e-02, -1.8025e-01, -6.7815e-01,  6.8125e-01,  2.6780e-01,\n",
      "         -1.2983e+00,  7.2077e-01,  1.3293e+00,  2.5311e-01,  5.2494e-01,\n",
      "         -1.4525e+00, -2.7671e-01, -4.8103e-01, -9.9362e-01, -1.1428e-01,\n",
      "          1.2033e+00, -4.0554e-01,  7.4388e-01,  9.8874e-01,  1.0805e+00,\n",
      "         -8.7663e-01,  1.5076e-01,  3.3455e-01, -9.4930e-01, -3.7420e-01,\n",
      "         -5.4317e-01,  1.8055e-01,  1.4846e+00, -1.2499e-02,  3.6847e-01,\n",
      "          1.7667e-01, -6.0996e-01, -6.3694e-01,  3.8995e-01,  1.7439e-01,\n",
      "          7.8642e-01,  1.5289e+00,  1.1468e+00,  1.0751e+00,  1.1781e-01,\n",
      "          1.2791e+00, -6.5682e-01, -1.3189e+00, -1.0855e+00,  8.6579e-01,\n",
      "          2.6453e-01, -1.0408e+00, -4.6901e-01, -4.8194e-01,  1.4052e+00,\n",
      "          1.0259e-01, -3.1057e-01, -5.7743e-01, -6.0951e-01, -3.3916e-01,\n",
      "         -1.9673e-01, -9.1445e-01, -8.0649e-01,  3.4176e-02, -4.9650e-02,\n",
      "         -2.1848e-01,  7.2256e-01,  7.8479e-01, -1.1299e+00, -1.1847e+00,\n",
      "         -2.4169e-02, -1.1446e-01, -3.6775e-01,  8.2433e-01,  8.8071e-01,\n",
      "         -3.4107e-01,  6.4769e-01, -3.0454e-01, -1.0259e+00,  1.5975e-01,\n",
      "         -6.2286e-01,  1.0777e-01, -2.5937e-01,  9.9342e-01, -4.5901e-01,\n",
      "         -1.2876e+00, -1.8518e-01, -5.1734e-01,  5.3465e-01,  5.8413e-01,\n",
      "          4.3348e-01, -1.2576e+00,  1.0642e-01,  1.0428e+00,  1.3208e-01,\n",
      "          7.6130e-01, -1.2414e+00, -8.5417e-02, -8.8439e-01, -4.0116e-01,\n",
      "          1.0803e+00, -2.0238e-01,  7.1424e-01, -1.5055e-01, -9.1289e-01,\n",
      "         -4.7191e-01,  7.2196e-01, -8.4662e-01,  1.9771e-01, -6.4393e-01,\n",
      "          4.7057e-01, -4.0980e-01,  5.8424e-02,  3.8889e-01, -3.4223e-01,\n",
      "         -1.1852e+00, -1.3812e+00,  3.4931e-01,  4.7213e-01, -3.6945e-01,\n",
      "          7.5271e-01, -1.0213e+00, -1.0307e-01, -9.1145e-01, -8.1439e-01,\n",
      "         -5.1228e-01,  3.0782e-01,  2.2749e+00,  2.7364e-01, -4.8049e-02,\n",
      "          9.8131e-01,  1.0277e+00,  2.9348e-01,  2.0049e-01, -4.0895e-02,\n",
      "          1.0486e+00,  9.7127e-01, -3.2769e-02]], grad_fn=<CatBackward>)\n",
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -1.1723e+00,  2.3573e-02,  1.1856e+00,\n",
      "         -8.5422e-01, -8.9325e-01, -3.3991e-01, -7.2060e-02,  1.0866e+00,\n",
      "         -6.7398e-01,  9.3840e-01, -9.8908e-01, -1.6452e+00, -2.6995e-01,\n",
      "          4.8602e-01, -1.7519e+00,  7.9274e-01, -6.0133e-01,  7.2332e-01,\n",
      "         -3.9212e-01, -7.7101e-02,  1.7570e+00, -1.1990e+00,  7.9673e-01,\n",
      "          1.1428e+00, -3.1973e-01, -6.0894e-01,  6.6357e-01,  2.2914e+00,\n",
      "         -1.3100e-01, -5.6774e-01,  1.5216e+00,  1.5095e+00, -1.1044e+00,\n",
      "         -1.3798e+00,  1.1786e+00,  1.0798e+00,  1.1484e+00,  4.7308e-01,\n",
      "          9.6668e-01,  9.4255e-02, -5.0975e-01, -1.4433e-01, -8.9043e-02,\n",
      "         -1.4640e+00,  1.1395e+00,  1.1900e-01,  4.2808e-01, -5.6041e-01,\n",
      "         -1.8714e-02,  2.2924e-01,  1.8924e+00,  9.0358e-01, -5.9261e-01,\n",
      "         -1.0587e+00, -1.5071e-01,  8.3437e-02, -5.5269e-01, -2.6167e-01,\n",
      "          4.3624e-01, -6.0987e-01,  1.4460e+00,  9.9812e-02,  5.6888e-01,\n",
      "          2.2712e-01,  1.4274e+00, -1.2108e+00, -5.2010e-02,  1.5954e-01,\n",
      "         -6.2900e-01,  5.1123e-01,  1.3757e+00, -1.6354e+00,  1.9243e-01,\n",
      "         -1.0646e+00,  5.2548e-01, -1.5116e-01,  6.7149e-01, -2.0972e-01,\n",
      "          5.4147e-01, -1.0220e+00, -5.1922e-01, -1.5373e+00,  1.0401e+00,\n",
      "         -1.9116e-01,  8.4503e-01, -8.0682e-01, -1.0220e+00, -7.4441e-02,\n",
      "          1.2291e+00, -1.4517e+00,  6.3101e-01,  1.4829e-01,  1.4019e+00,\n",
      "          9.2249e-01,  5.7516e-01,  7.4769e-01, -1.4557e-01,  4.0282e-01,\n",
      "         -1.0100e-01, -7.7795e-01,  5.9548e-01,  1.7058e+00, -5.4068e-01,\n",
      "          1.9184e+00, -2.5282e-01, -4.7041e-01,  6.9842e-01, -8.8754e-01,\n",
      "          4.8333e-01, -1.0072e+00,  4.7502e-02,  6.0545e-02,  7.0749e-01,\n",
      "          1.9272e-01,  9.4856e-01, -2.3628e-01,  3.4345e-01,  4.5628e-02,\n",
      "          4.2474e-01,  4.7216e-02,  6.9181e-01,  2.5453e+00, -4.6087e-01,\n",
      "         -5.2527e-01, -2.8796e-02, -1.0114e+00,  2.5396e-01,  3.5185e-01,\n",
      "         -1.3896e+00,  8.2981e-01,  1.1502e+00,  1.4859e-01,  9.3504e-01,\n",
      "         -1.8107e+00, -2.2527e-01, -3.2658e-01, -1.3782e+00, -3.4930e-01,\n",
      "          1.4003e+00, -5.9557e-01,  7.2037e-01,  9.9102e-01,  1.4465e+00,\n",
      "         -1.0324e+00,  4.0348e-01,  4.7708e-01, -1.0521e+00, -2.3282e-01,\n",
      "         -1.0784e+00,  3.2205e-01,  2.2032e+00, -1.0120e-01,  7.1030e-01,\n",
      "         -1.4256e-01, -3.2018e-01, -9.7706e-01,  7.1271e-01,  2.5441e-01,\n",
      "          6.9973e-01,  1.6312e+00,  1.1465e+00,  8.7034e-01,  1.7938e-02,\n",
      "          1.3458e+00, -1.0666e+00, -1.5576e+00, -9.7041e-01,  1.0304e+00,\n",
      "          4.1473e-01, -1.1094e+00, -5.1066e-01, -5.4286e-01,  1.4525e+00,\n",
      "         -7.2574e-02, -3.4583e-01, -6.8513e-01, -8.7917e-01, -4.9479e-01,\n",
      "         -4.3796e-01, -7.8697e-01, -7.2398e-01,  4.4900e-01, -8.7873e-02,\n",
      "         -4.6648e-01,  7.2095e-01,  8.5806e-01, -1.1936e+00, -1.2559e+00,\n",
      "          3.6040e-02,  2.2550e-01, -2.0109e-01,  1.0767e+00,  7.1531e-01,\n",
      "         -3.8859e-01,  9.7796e-01, -4.8342e-02, -9.1806e-01,  5.1591e-02,\n",
      "         -7.7440e-01,  2.8365e-01, -3.1376e-01,  1.1173e+00, -2.5497e-01,\n",
      "         -1.7492e+00,  1.0033e-01, -1.0308e+00,  3.7886e-01,  8.7885e-01,\n",
      "          6.4588e-01, -1.4428e+00,  3.8143e-01,  1.0342e+00, -1.0301e-01,\n",
      "          8.0322e-01, -1.4690e+00, -3.6205e-01, -1.3921e+00, -4.9494e-01,\n",
      "          1.9240e+00, -3.0726e-01,  6.9311e-01, -3.6334e-01, -1.2516e+00,\n",
      "         -4.9807e-01,  6.8272e-01, -6.8015e-01, -4.8013e-02, -7.5551e-01,\n",
      "          7.3808e-01, -6.6118e-01, -2.5353e-01,  4.0849e-01, -4.5224e-01,\n",
      "         -1.0734e+00, -1.5369e+00,  1.3045e-01,  5.1338e-01, -5.1645e-02,\n",
      "          8.8677e-01, -1.1773e+00, -3.3693e-01, -9.1351e-01, -7.5913e-01,\n",
      "         -7.6862e-01,  5.5482e-01,  2.5532e+00,  6.4212e-01,  4.5936e-02,\n",
      "          9.3556e-01,  1.0732e+00, -6.5674e-02,  3.2637e-01, -1.7841e-01,\n",
      "          1.2111e+00,  1.4870e+00,  6.6005e-02]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "for next_vertex in {2, 5, 8}:\n",
    "    print(torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9720,  0.0779,  0.7608,  ...,  0.4971,  0.4958,  0.1762],\n",
       "        [-0.9927,  0.0789,  0.7894,  ...,  0.7697,  0.9993,  0.1767]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-505-2cee602f4b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaths_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_emb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "[torch.cat([get_states_emb(paths, graph_emb), graph_emb[next_vertex].unsqueeze(0)]) for next_vertex in {2, 5, 8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_states_emb([[1,2,3]], graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for i in x:\n",
    "    print(torch.cat([i, next_emb]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_embs = get_states_emb(paths, graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = agent.value(paths_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1743],\n",
       "        [-0.2147]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.9720,  0.0779,  0.7608,  ...,  0.7697,  0.9993,  0.1767],\n",
      "        [-0.9720,  0.0779,  0.7608,  ...,  1.0486,  0.9713, -0.0328],\n",
      "        [-0.9720,  0.0779,  0.7608,  ...,  1.2111,  1.4870,  0.0660]],\n",
      "       grad_fn=<StackBackward>), tensor([[-0.9927,  0.0789,  0.7894,  ...,  0.9108,  0.9007,  0.0928],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  0.4971,  0.4958,  0.1762],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  1.0486,  0.9713, -0.0328],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  1.2688,  1.7698,  0.0705]],\n",
      "       grad_fn=<StackBackward>)]\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "for i, path in enumerate(paths):\n",
    "    next_embs = []\n",
    "    for next_vertex in p.edges[path[-1]]:\n",
    "        next_vertex_emb = graph_emb[next_vertex]\n",
    "        next_embs.append(torch.cat([paths_embs[i], next_vertex_emb]))\n",
    "    states.append(torch.stack(next_embs))\n",
    "print(states)\n",
    "predicts = []\n",
    "for i in states:\n",
    "    predicts.append(sm(log_reg(i)).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3215, 0.3476, 0.3309], grad_fn=<ViewBackward>),\n",
       " tensor([0.2441, 0.2059, 0.2333, 0.3167], grad_fn=<ViewBackward>)]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-553-07342f11f5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'states'"
     ]
    }
   ],
   "source": [
    "torch.states.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {2, 5, 8, 11, 14},\n",
       "             2: {0, 1, 7, 8, 9, 14},\n",
       "             5: {0, 3, 7, 9, 12, 13},\n",
       "             8: {0, 2, 9, 13},\n",
       "             11: {0, 1, 10, 12, 13},\n",
       "             14: {0, 2, 3, 10, 12, 13},\n",
       "             1: {2, 3, 11, 13},\n",
       "             3: {1, 5, 6, 14},\n",
       "             13: {1, 4, 5, 6, 8, 10, 11, 12, 14},\n",
       "             7: {2, 4, 5},\n",
       "             9: {2, 5, 8},\n",
       "             6: {3, 12, 13},\n",
       "             4: {7, 12, 13},\n",
       "             12: {4, 5, 6, 11, 13, 14},\n",
       "             10: {11, 13, 14}})"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
