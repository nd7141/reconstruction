{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from pandas import DataFrame\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from utils_mcts import ReplayBuffer, PathsBuffer, get_states_emb, convert_to_walk\n",
    "from MCTS import MCTS\n",
    "from problem_mcts import GraphProblem, generate_erdos_renyi_problems, generate_regular_problems\n",
    "from network_mcts import Agent\n",
    "import time\n",
    "import nn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(P, source, target):\n",
    "    '''Replace last occurrence of source with source-target-source.'''\n",
    "    assert source in P\n",
    "    ix = len(P) - P[::-1].index(source)\n",
    "    return P[:ix] + [target, P[ix - 1]] + P[ix:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covering_walk(graph, source):\n",
    "    P = [0]  # supporting walk\n",
    "    S = [0]  # stack of nodes to check\n",
    "    node2anon = {source: 0}\n",
    "    anon2node = {0: source}\n",
    "    checked = dict()  # nodes that has been checked for edge\n",
    "    degrees = graph.degree()\n",
    "    while len(S) > 0:  # grow supporting walk in DFS manner\n",
    "        curr = S[-1]\n",
    "        x = max(P) + 1  # next node to check\n",
    "\n",
    "        # check if there is a node in the neighborhood that has not been explored yet\n",
    "        Ncurr = list(nx.neighbors(graph, anon2node[curr]))\n",
    "        if random.uniform(0, 1) < 0.99:\n",
    "            random.shuffle(Ncurr)  # option 1: random order\n",
    "        else:\n",
    "            Ncurr = sorted(Ncurr, key=lambda v: degrees[v], reverse=True)  # option 2: top-degree\n",
    "            # Ncurr = sorted(Ncurr, key=lambda v: degrees[v], reverse=False)  # option 3: low-degree\n",
    "        # print(anon2node[curr], Ncurr)\n",
    "        for neighbor in Ncurr:\n",
    "            if neighbor in node2anon:\n",
    "                continue  # already visited\n",
    "            else:\n",
    "                node2anon[neighbor] = x\n",
    "                anon2node[x] = neighbor\n",
    "                S.append(x)\n",
    "                checked.setdefault(curr, set()).add(x)\n",
    "                P = replace(P, curr, x)  # move to it\n",
    "                break\n",
    "        else:\n",
    "            S.pop()  # move back in the stack\n",
    "\n",
    "        for u in range(x-1, curr, -1):  # u is already in the supporting walk\n",
    "            # check if there is connection to already discovered nodes\n",
    "            if u not in checked[curr]:  # see if we already checked this edge\n",
    "                if anon2node[u] in graph[anon2node[curr]]:\n",
    "                    P = replace(P, curr, u)\n",
    "                checked.setdefault(curr, set()).add(u)\n",
    "\n",
    "    cover = [anon2node[v] for v in P]\n",
    "    return cover, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "NUM_PROBLEMS = 50\n",
    "NUM_EPISODES = 50\n",
    "BATCH_SIZE = 32\n",
    "NUM_MCSIMS = 50\n",
    "NUM_UPDATES = 5\n",
    "NUM_VERTICES = 15\n",
    "DEGREE = 6\n",
    "CPUCT = 1.0\n",
    "THRESHOLD = 0.75\n",
    "PATHS_BUFFER_CAPACITY = 1000\n",
    "REPLAY_BUFFER_CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate regular train graphs (n=15, d=6)\n",
    "problem_maker = generate_erdos_renyi_problems(num_vertices=NUM_VERTICES, edge_prob=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent\n",
    "agent = Agent(hid_size=256, gcn_size=256, vertex_emb_size=64, num_vertices=NUM_VERTICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize buffers\n",
    "path_buffer = PathsBuffer(capacity=PATHS_BUFFER_CAPACITY, threshold=THRESHOLD)\n",
    "train_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss stats\n",
    "pi_losses_history = []\n",
    "v_losses_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [next(problem_maker) for i in range(NUM_PROBLEMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'path_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-50551e0021c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             mcts = MCTS(game=problem, nnet=agent, graph_emb=graph_emb,\n\u001b[0;32m---> 24\u001b[0;31m                         numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mrandom_walk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_length' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for k in trange(len(problems)):\n",
    "    \n",
    "    problem = problems[k]\n",
    "\n",
    "    for vertex in problem.get_actions():\n",
    "\n",
    "        path_buffer.flush()\n",
    "    \n",
    "        PATH_LENGTH = 2*problem.num_edges + 1        \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "        for i in range(NUM_EPISODES):\n",
    "            \n",
    "            problem.path = [vertex]\n",
    "        \n",
    "            source = problem.get_state()[0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                graph_emb = agent.embed_graph(problem.edges)\n",
    "                \n",
    "            mcts = MCTS(game=problem, nnet=agent, graph_emb=graph_emb,\n",
    "                        numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n",
    "                \n",
    "            random_walk = [source]\n",
    "            checked = ddict(list)\n",
    "            stack = [source]\n",
    "            visited = {source}\n",
    "            ranks = {0: source} # to attempt to get maximal cover (possible to do without rank, but then no guarantees on maximality)\n",
    "            revranks = {source: 0}\n",
    "            \n",
    "            trainExamples = []\n",
    "            \n",
    "            while len(stack) > 0:\n",
    "                last = stack[-1]\n",
    "                lastrank = revranks[last]\n",
    "                maxrank = max(ranks.keys()) + 1\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pi = mcts.getActionProb(random_walk[:])\n",
    "                \n",
    "                Nlast = np.argsort(pi)\n",
    "\n",
    "                # going in depth\n",
    "                for neighbor in Nlast:\n",
    "                    if neighbor not in visited: # found new node, then add it to the walk\n",
    "                        trainExamples.append([random_walk, pi, None])\n",
    "                        random_walk.append(neighbor)\n",
    "                        stack.append(neighbor)\n",
    "                        checked[last].append(neighbor)\n",
    "                        visited.add(neighbor)\n",
    "                        ranks[maxrank] = neighbor\n",
    "                        revranks[neighbor] = maxrank\n",
    "                        break\n",
    "                else: # we didn't find any new neighbor and rollback\n",
    "                    stack.pop()\n",
    "                    if len(stack) > 0:\n",
    "                        random_walk.append(stack[-1])\n",
    "                        checked[last].append(stack[-1])\n",
    "\n",
    "                # interconnecting nodes that are already in walk\n",
    "                for r in range(maxrank-1, lastrank+1, -1):\n",
    "                    node = ranks[r]\n",
    "                    if node not in checked[last] and node in Nlast:\n",
    "                        checked[last].append(node)\n",
    "                        random_walk.extend([node, last])\n",
    "        \n",
    "            path_buffer.push(random_walk)\n",
    "            if len(path_buffer) >= 10: \n",
    "                r = path_buffer.rank_path(random_walk)\n",
    "                for x in trainExamples:\n",
    "                    x[-1] = r\n",
    "                train_buffer.push(trainExamples)\n",
    "            \n",
    "        if len(train_buffer) >= BATCH_SIZE:\n",
    "            for i in range(NUM_UPDATES):\n",
    "                batch = train_buffer.sample(BATCH_SIZE)\n",
    "                paths, pis, vs = zip(*batch)\n",
    "                embs = get_states_emb(paths, graph_emb)\n",
    "\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                out_pi, out_v = agent(embs)\n",
    "                loss_pi = -torch.sum(target_pis*out_pi)/target_pis.size()[0]\n",
    "                loss_v = torch.sum((target_vs-out_v.view(-1))**2)/target_vs.size()[0]\n",
    "                total_loss = loss_pi + loss_v\n",
    "\n",
    "                pi_losses_history.append(loss_pi.item())\n",
    "                v_losses_history.append(loss_v.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if iteration % 5 == 0:\n",
    "                    clear_output(True)\n",
    "                    plt.figure(figsize=[12, 6])\n",
    "                    plt.subplot(1,2,1)\n",
    "                    plt.title('Policy error'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(pi_losses_history)), pi_losses_history, alpha=0.1)\n",
    "                    plt.plot(moving_average(pi_losses_history, span=100, min_periods=100))\n",
    "\n",
    "                    plt.subplot(1,2,2)\n",
    "                    plt.title('Value error'); plt.grid()\n",
    "                    plt.scatter(np.arange(len(v_losses_history)), v_losses_history, alpha=0.1)\n",
    "                    plt.plot(moving_average(v_losses_history, span=10, min_periods=10))\n",
    "                    plt.show()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next(problem_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.path = [random.sample(list(p.edges.keys()), 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)\n",
    "path_length = 2*p.num_edges+1\n",
    "mcts = MCTS(game=p, nnet=agent, graph_emb=graph_emb,\n",
    "                    numMCTSSims=NUM_MCSIMS, cpuct=CPUCT, path_length=path_length)\n",
    "path = p.get_state()\n",
    "while len(path) != path_length:\n",
    "    with torch.no_grad():\n",
    "        pi = mcts.getActionProb(path)\n",
    "    vertex = np.random.choice(len(pi), p=pi)\n",
    "    path = p.get_next_state(path, vertex)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([1, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(256, hidden_size= 256, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(256, 256, batch_first=True)\n"
     ]
    }
   ],
   "source": [
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in x:\n",
    "    l.append(graph_emb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.7244,  0.2185,  0.3596, -0.1095, -0.4744,  0.6388,  0.2494, -0.8377,\n",
       "          0.5006, -0.0812,  0.3653,  0.3609, -0.4720,  0.5132,  0.3926, -0.1603,\n",
       "          0.2755, -0.2794, -0.1765,  0.3675, -0.2131,  0.6642,  0.3640,  0.6578,\n",
       "          0.0418, -0.2552, -0.1864,  0.0288,  0.3387,  0.9063, -0.5579, -0.1466,\n",
       "         -0.5701,  0.0792, -0.2241, -0.2781,  0.4577,  0.1009,  0.5546, -0.3852,\n",
       "         -0.4224, -0.3637,  0.6582, -0.4686,  0.3631,  0.3806, -0.2212, -0.1533,\n",
       "          0.0481, -0.0058, -1.2877,  0.3794, -0.4454,  0.6694,  0.6999, -0.3055,\n",
       "          0.4080, -0.5932, -0.2140,  0.1772,  0.1573,  0.0113, -0.1725,  0.2193,\n",
       "         -0.4351, -0.5070, -0.2644,  0.0465,  0.3909,  0.2895, -0.0799, -0.1596,\n",
       "         -0.5890,  0.7873, -0.0486, -0.0918,  0.3417, -0.5025, -0.2667, -0.1139,\n",
       "          0.2465,  0.4984, -0.1131,  0.3431, -0.5221, -0.2689,  0.4080, -0.3041,\n",
       "         -0.0120,  0.5861, -0.1446,  0.2725, -0.0121,  0.6641, -0.4134,  0.0799,\n",
       "          0.1063,  0.1478, -0.1255,  0.3726,  0.0150,  0.3702,  0.3965,  0.5434,\n",
       "          0.3101, -0.5531,  0.1583,  0.0737, -0.2691, -0.2308,  0.5094,  0.2346,\n",
       "          0.1821,  0.3362, -0.1873,  0.0392,  0.4873, -0.4415, -0.6707, -0.4434,\n",
       "          0.0086, -0.2639,  0.0042,  0.5447, -0.4130,  0.6777, -0.3000, -0.1908,\n",
       "         -0.1438,  0.2234, -0.1945, -0.7022, -0.0444,  0.5378,  0.0421, -0.0285,\n",
       "          0.2736,  0.5125,  0.0050, -0.0079, -0.2203, -0.1087,  0.1083, -0.5373,\n",
       "          0.0581,  0.0170, -0.1755, -0.5087,  0.5512,  0.1128,  0.9071, -0.0304,\n",
       "          0.4970,  0.0563,  0.3696, -0.2489, -1.1382, -0.0618, -0.0348,  0.3277,\n",
       "         -0.6858, -0.1760,  0.6266, -0.9198, -0.1608,  0.3261, -0.2478,  0.3920,\n",
       "          0.1891, -0.5152, -0.6392, -0.1063,  0.5096, -0.3226,  0.1378, -0.7633,\n",
       "         -0.1413, -0.6838, -0.0701,  0.6568,  0.1270, -0.0241, -0.6459,  0.2387,\n",
       "          0.2294, -0.2304,  0.6488,  0.2019, -0.4436, -0.1158,  0.3528, -0.1504,\n",
       "         -0.4591, -0.0705,  0.5744,  0.1036,  0.1086,  0.3170,  0.5712,  0.3376,\n",
       "          0.2383,  0.1685,  0.0436,  0.2450,  0.3582,  0.0108, -0.5914, -0.0966,\n",
       "          0.0945,  0.4856,  0.3125,  0.6621, -0.5637, -0.3747,  0.4244, -0.1509,\n",
       "          0.0825, -0.0915,  0.0793, -0.2636,  0.7481, -0.0778,  0.1748, -0.5024,\n",
       "         -0.6648,  0.2188,  0.5138, -0.4361, -0.5197, -0.3261, -0.3194,  0.2766,\n",
       "          0.5643, -0.1361, -0.0724,  0.1885, -0.0867, -0.0289,  0.0754,  0.1488,\n",
       "         -0.2543, -0.1457,  0.7227,  0.4909, -0.0735,  0.4583, -0.6466, -0.1452,\n",
       "          0.6620, -0.4182,  0.1307,  0.2679, -0.4038, -0.1213, -0.0361, -0.2045],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([-0.5270,  0.1055,  0.0569, -0.0956, -0.3629,  0.4572, -0.1211, -0.7286,\n",
       "          0.1708, -0.1828,  0.3622,  0.1190, -0.3543,  0.4002,  0.5598, -0.1351,\n",
       "          0.3255, -0.4557, -0.3375,  0.3881, -0.2352,  0.3863,  0.2512,  0.6069,\n",
       "          0.3262, -0.3630, -0.4547, -0.3203,  0.1493,  0.3881, -0.5239, -0.1970,\n",
       "         -0.4078,  0.1144, -0.1590, -0.0614,  0.4130,  0.2451,  0.3855, -0.3598,\n",
       "         -0.0208, -0.3808,  0.4214, -0.3811,  0.2614,  0.3358, -0.5333, -0.0876,\n",
       "         -0.2308, -0.1607, -0.6586,  0.4808, -0.1891,  0.3684,  0.4858, -0.0337,\n",
       "          0.1629, -0.6063, -0.3858,  0.2474, -0.0185, -0.1578, -0.3717,  0.3363,\n",
       "         -0.2918, -0.3840, -0.0040, -0.0643,  0.3938,  0.1642, -0.1480, -0.0554,\n",
       "         -0.5797,  0.4935, -0.0665, -0.1562,  0.4670, -0.3933, -0.0132,  0.1581,\n",
       "          0.0171,  0.2017, -0.0631,  0.4256, -0.1175, -0.0308,  0.3409,  0.0906,\n",
       "          0.1408,  0.5446,  0.0159,  0.4079, -0.1854,  0.6471, -0.2612, -0.2015,\n",
       "         -0.1668,  0.0874, -0.1032,  0.0090,  0.0490,  0.3769,  0.4310,  0.4794,\n",
       "          0.2156, -0.3520,  0.1247, -0.0611, -0.5136, -0.4096,  0.7721,  0.1711,\n",
       "          0.3193,  0.4036,  0.1525,  0.0173,  0.2211, -0.4835, -0.4124, -0.3647,\n",
       "          0.1570, -0.4575, -0.0267,  0.4476, -0.2834,  0.4321,  0.0966, -0.4186,\n",
       "         -0.0210,  0.2863, -0.3794, -0.3619,  0.1568,  0.0207,  0.0746,  0.1305,\n",
       "          0.2177,  0.0770, -0.0075,  0.0456, -0.1074, -0.1918, -0.0817, -0.6915,\n",
       "          0.3082,  0.3424, -0.2182, -0.0883,  0.3851,  0.1073,  0.7868, -0.0050,\n",
       "          0.5693, -0.0498,  0.0833,  0.1247, -0.9824,  0.0865,  0.3588,  0.1226,\n",
       "         -0.4859, -0.3005,  0.3582, -0.9469, -0.2291,  0.1061, -0.2131,  0.0257,\n",
       "          0.1548, -0.7028, -0.1032, -0.1947,  0.1687, -0.0392, -0.0722, -0.5456,\n",
       "          0.1131, -0.5138, -0.0606,  0.3575,  0.3782,  0.1745,  0.0029,  0.3572,\n",
       "          0.1572, -0.2089,  0.2746,  0.2685, -0.3089, -0.3212,  0.1600, -0.2054,\n",
       "         -0.0961, -0.1366,  0.5654, -0.3977,  0.0391,  0.2016,  0.3799,  0.1287,\n",
       "          0.1853,  0.1421,  0.1629,  0.4559,  0.2907,  0.2962, -0.6351,  0.2284,\n",
       "          0.2170,  0.4177,  0.0264,  0.4314, -0.1457, -0.2140,  0.3287, -0.0629,\n",
       "          0.2801,  0.1076, -0.1105, -0.3439,  0.6127, -0.1844, -0.1479, -0.2708,\n",
       "         -0.3545,  0.0785,  0.4989, -0.3903, -0.1057, -0.3219, -0.4416, -0.1171,\n",
       "          0.4754, -0.1807,  0.0703, -0.1971, -0.2037, -0.0577, -0.1256, -0.0909,\n",
       "         -0.1792, -0.1597,  0.5338,  0.3343,  0.1716,  0.3755, -0.7684,  0.2172,\n",
       "          0.4311, -0.3551, -0.0110,  0.0599,  0.1931, -0.1807,  0.1780, -0.1330],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([-1.3411,  0.5691,  0.4539, -0.3046, -0.9237,  1.1116, -0.5621, -1.5609,\n",
       "          0.2574,  0.0950,  0.9736,  0.3364, -1.0446,  1.2248,  1.6642, -0.4583,\n",
       "          0.8717, -1.2553, -0.7956,  1.3402, -0.7775,  1.0645,  0.5144,  1.2538,\n",
       "          0.3570, -0.8826, -1.1588, -0.5704,  0.3622,  1.2871, -1.3833, -0.9006,\n",
       "         -1.2887, -0.0614, -0.1276, -0.1141,  0.9442,  0.7442,  0.9875, -1.0450,\n",
       "         -0.1037, -1.0840,  0.7683, -0.7830,  0.5790,  0.8912, -1.3147, -0.6012,\n",
       "         -0.1974, -0.0759, -1.9387,  1.1089, -0.4293,  1.3451,  1.2539, -0.1804,\n",
       "          0.2450, -1.7748, -0.7163,  0.9095, -0.1432, -0.1924, -1.1370,  0.5453,\n",
       "         -0.6151, -0.9883, -0.1433, -0.3907,  1.1217,  0.3567, -0.4782, -0.3093,\n",
       "         -1.6522,  1.1570, -0.2183, -0.4729,  1.3595, -1.0348,  0.2434,  0.5034,\n",
       "         -0.0181,  0.6573, -0.1197,  0.7921, -0.6531, -0.0170,  0.5453,  0.3188,\n",
       "          0.3238,  1.3941, -0.0253,  0.9748, -0.3865,  1.5463, -0.4557, -0.5000,\n",
       "         -0.2345,  0.4452, -0.1650,  0.0691,  0.0757,  0.6589,  0.9217,  1.0356,\n",
       "          0.2228, -0.6348,  0.0640, -0.0663, -1.1156, -0.7828,  1.7918, -0.1142,\n",
       "          0.7514,  0.9677,  0.5123,  0.0487,  0.6830, -0.9149, -1.6241, -1.1692,\n",
       "          0.2760, -1.3601, -0.1422,  0.6426, -0.7225,  1.4378, -0.1766, -0.7827,\n",
       "          0.2679,  0.8644, -0.7864, -0.7628,  0.1428,  0.0915,  0.2125,  0.1144,\n",
       "          0.8537,  0.1511,  0.2101,  0.0413, -0.0779, -0.3437, -0.1834, -1.6365,\n",
       "          0.7934,  1.0661, -0.4511, -0.7270,  0.6837, -0.0491,  1.8393, -0.1181,\n",
       "          2.0262, -0.4402,  0.3421,  0.6508, -1.9891,  0.1803,  1.0892,  0.3984,\n",
       "         -1.2966, -0.9619,  0.8237, -2.2944, -0.1413,  0.2030, -0.4698,  0.2122,\n",
       "         -0.0697, -1.5959, -0.3936, -0.7434,  0.5492,  0.1703, -0.1565, -1.4185,\n",
       "          0.0574, -1.3872, -0.5351,  0.8490,  0.8846,  0.1244, -0.0642,  0.8475,\n",
       "          0.7152, -0.7667,  0.8499,  0.4620, -0.5407, -0.3546,  0.2676, -0.3033,\n",
       "          0.1186, -0.4344,  1.3510, -0.5820,  0.2604,  0.1418,  0.8009,  0.2738,\n",
       "          0.6107,  0.4129,  0.2579,  0.9236,  0.8237,  0.7088, -1.7246,  0.5036,\n",
       "          0.8252,  1.1857,  0.3914,  1.2329, -0.3590, -0.3800,  0.7929, -0.3752,\n",
       "          0.8708,  0.6654, -0.0519, -1.0107,  1.7609, -0.8537, -0.5140, -0.7938,\n",
       "         -0.8722,  0.7640,  0.8266, -1.1201, -0.5656, -1.1682, -0.9833, -0.0164,\n",
       "          0.9743, -0.4317,  0.0149, -0.4938, -0.2202, -0.2705, -0.4835, -0.0957,\n",
       "         -0.9434,  0.0298,  1.5774,  0.9479,  0.5285,  1.0244, -2.0400,  0.6204,\n",
       "          1.3631, -1.1889, -0.2927, -0.0026,  0.2614, -0.4401,  0.8559, -0.4931],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([-1.4175,  0.6699,  0.6074, -0.5103, -1.3075,  1.3001, -0.4777, -1.5501,\n",
       "          0.4283,  0.0543,  1.1169,  0.4142, -1.1771,  1.4754,  1.6437, -0.6555,\n",
       "          0.9082, -1.2936, -0.7924,  1.3337, -0.8694,  1.3201,  0.6914,  1.3664,\n",
       "          0.0340, -0.8117, -1.0239, -0.3674,  0.5057,  1.6150, -1.3584, -0.5973,\n",
       "         -1.4244, -0.0129, -0.1953, -0.4784,  1.0324,  0.8877,  1.0821, -1.0577,\n",
       "         -0.3500, -1.2942,  0.7636, -0.9740,  0.6144,  1.0292, -1.1772, -0.7529,\n",
       "         -0.1013, -0.0082, -2.4579,  1.1507, -0.3627,  1.6532,  1.4932, -0.5111,\n",
       "          0.3539, -1.8384, -0.7800,  0.8399, -0.1016, -0.0030, -1.0381,  0.5785,\n",
       "         -0.6216, -1.0506, -0.3935, -0.4192,  1.1637,  0.5212, -0.6670, -0.2861,\n",
       "         -1.7114,  1.5182, -0.1304, -0.3011,  1.1163, -1.1680,  0.1792,  0.4135,\n",
       "          0.1689,  0.9436, -0.1571,  0.8872, -1.0620, -0.0751,  0.6980, -0.0762,\n",
       "          0.1265,  1.5625, -0.2762,  0.9601, -0.3139,  1.4980, -0.5531, -0.5039,\n",
       "         -0.1752,  0.4604, -0.2988,  0.2297,  0.0764,  0.6228,  0.8000,  1.0614,\n",
       "          0.3852, -0.6250,  0.0124,  0.0796, -1.0604, -0.7958,  1.8735, -0.0578,\n",
       "          0.7032,  0.8912,  0.4229, -0.1527,  0.9522, -1.0985, -1.8243, -1.2133,\n",
       "          0.1816, -1.4981, -0.1443,  0.8561, -0.8689,  1.5816, -0.4261, -0.9318,\n",
       "          0.1661,  0.8756, -0.6529, -1.0476, -0.0613,  0.3187,  0.2406, -0.1014,\n",
       "          1.1021,  0.1897,  0.1303,  0.1837, -0.2690, -0.3177, -0.2160, -1.6819,\n",
       "          0.8733,  1.0689, -0.6034, -0.9401,  0.9186,  0.0371,  2.1104, -0.3182,\n",
       "          1.9157, -0.3778,  0.6245,  0.5763, -2.0933,  0.1229,  0.8297,  0.4877,\n",
       "         -1.5169, -1.0226,  0.9648, -2.1379, -0.0736,  0.3507, -0.6899,  0.3379,\n",
       "          0.2080, -1.5495, -0.9251, -0.6607,  0.6563,  0.0207, -0.1701, -1.2538,\n",
       "          0.1467, -1.6356, -0.5809,  0.8674,  0.8587, -0.1258, -0.5899,  0.7278,\n",
       "          0.7207, -0.8325,  0.8935,  0.6350, -0.6484, -0.2751,  0.4766, -0.0935,\n",
       "         -0.1898, -0.3847,  1.4443, -0.3966,  0.4203,  0.3533,  0.8389,  0.3559,\n",
       "          0.6597,  0.5868,  0.3113,  0.8171,  1.0085,  0.4800, -1.8910,  0.3267,\n",
       "          1.0202,  1.1433,  0.6176,  1.5641, -0.4651, -0.5062,  0.7033, -0.4708,\n",
       "          0.9034,  0.6485,  0.0587, -1.0960,  1.7327, -0.6871, -0.4639, -1.1171,\n",
       "         -1.0313,  0.7160,  0.7542, -1.1883, -0.9525, -1.1107, -0.8771,  0.3787,\n",
       "          0.9575, -0.3462,  0.1571, -0.3252,  0.0089, -0.2483, -0.4572, -0.2515,\n",
       "         -1.0507, -0.0683,  1.8912,  0.8788,  0.5617,  1.3605, -1.9906,  0.4585,\n",
       "          1.6034, -1.0716, -0.1661,  0.0313, -0.3494, -0.2983,  0.6935, -0.4998],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([-6.0323e-01,  1.0227e-01,  1.5441e-01, -2.6338e-01, -6.1105e-01,\n",
       "          5.9619e-01, -9.2031e-02, -6.8749e-01,  2.8860e-01, -2.7402e-01,\n",
       "          4.7378e-01,  1.1675e-01, -3.9739e-01,  4.2441e-01,  5.9125e-01,\n",
       "         -2.2707e-01,  2.6913e-01, -4.2688e-01, -3.4061e-01,  2.8618e-01,\n",
       "         -3.7677e-01,  5.7806e-01,  3.0021e-01,  7.6899e-01,  1.3370e-01,\n",
       "         -3.8089e-01, -3.8079e-01, -2.5987e-01,  2.3750e-01,  5.2861e-01,\n",
       "         -5.0164e-01, -1.0034e-02, -4.8174e-01,  1.0962e-01, -1.4151e-01,\n",
       "         -2.8479e-01,  5.2560e-01,  3.0526e-01,  4.0751e-01, -3.1165e-01,\n",
       "         -1.3905e-01, -5.3286e-01,  3.6572e-01, -5.8449e-01,  3.2320e-01,\n",
       "          5.8224e-01, -5.0933e-01, -1.9965e-01, -2.0841e-01, -1.9363e-01,\n",
       "         -8.5163e-01,  3.7220e-01, -2.0659e-01,  5.8979e-01,  5.7755e-01,\n",
       "         -2.6668e-01,  1.4231e-01, -6.0564e-01, -4.2429e-01,  1.4395e-01,\n",
       "          4.2166e-02, -5.6379e-02, -3.5153e-01,  3.8907e-01, -2.5622e-01,\n",
       "         -3.5237e-01, -1.9423e-01, -1.1848e-01,  4.1324e-01,  2.9146e-01,\n",
       "         -2.4698e-01, -2.2454e-02, -6.5722e-01,  6.3241e-01, -2.8718e-02,\n",
       "         -3.3606e-02,  3.9367e-01, -3.9938e-01,  1.5437e-02,  1.2643e-01,\n",
       "          1.4825e-01,  3.9094e-01, -8.9723e-02,  4.0813e-01, -3.3617e-01,\n",
       "          3.0104e-02,  4.6084e-01, -1.2628e-01,  2.9422e-02,  5.9129e-01,\n",
       "         -2.1284e-04,  3.5976e-01, -2.0601e-01,  6.5694e-01, -3.2537e-01,\n",
       "         -2.3892e-01, -2.1988e-01,  1.4498e-01, -1.9680e-01,  1.6844e-02,\n",
       "         -1.5234e-03,  4.0664e-01,  4.7268e-01,  3.8138e-01,  3.0670e-01,\n",
       "         -2.9879e-01,  8.9875e-02,  1.5383e-01, -5.1129e-01, -5.1667e-01,\n",
       "          8.3332e-01,  2.0490e-01,  2.7585e-01,  3.8510e-01,  2.0360e-01,\n",
       "         -7.6730e-02,  3.0352e-01, -6.3635e-01, -4.5242e-01, -3.8083e-01,\n",
       "          1.2271e-01, -5.4102e-01, -5.7436e-02,  6.2901e-01, -3.4173e-01,\n",
       "          4.8749e-01, -5.4455e-04, -5.1858e-01, -1.7111e-01,  3.1509e-01,\n",
       "         -2.7800e-01, -4.3043e-01,  4.1459e-02,  1.2456e-01,  7.5531e-02,\n",
       "         -8.0094e-03,  3.9036e-01,  6.1431e-02, -3.9954e-02,  2.2712e-01,\n",
       "         -2.0417e-01, -1.6803e-01, -1.0241e-01, -7.7229e-01,  4.2056e-01,\n",
       "          3.1900e-01, -3.3681e-01, -2.0322e-01,  4.1717e-01,  1.2777e-01,\n",
       "          9.8979e-01, -5.1146e-02,  5.1983e-01, -2.3263e-02,  2.2244e-01,\n",
       "          1.4130e-01, -1.0455e+00, -1.2696e-02,  2.4441e-01,  3.0665e-01,\n",
       "         -6.3279e-01, -2.9687e-01,  3.3474e-01, -7.7153e-01, -2.3207e-01,\n",
       "          1.6549e-01, -2.7563e-01,  2.1503e-02,  2.5101e-01, -7.5311e-01,\n",
       "         -3.5299e-01, -6.8415e-02,  2.0581e-01, -3.0293e-02, -2.3625e-01,\n",
       "         -3.3706e-01,  1.2732e-01, -5.9840e-01, -1.3107e-01,  3.8166e-01,\n",
       "          3.8674e-01,  9.4083e-02, -1.8153e-01,  2.8901e-01,  1.4988e-01,\n",
       "         -2.5122e-01,  1.8971e-01,  2.6627e-01, -2.8791e-01, -2.5743e-01,\n",
       "          1.5360e-01, -1.7040e-01, -2.1818e-01, -1.3445e-01,  5.5245e-01,\n",
       "         -2.8198e-01,  1.1242e-01,  2.9562e-01,  4.4519e-01,  2.2576e-01,\n",
       "          2.7488e-01,  1.8075e-01,  1.9175e-01,  3.8340e-01,  4.4643e-01,\n",
       "          1.7346e-01, -6.9328e-01,  1.3414e-01,  3.6569e-01,  4.0355e-01,\n",
       "          2.0973e-01,  5.5695e-01, -2.0905e-01, -2.8268e-01,  2.7860e-01,\n",
       "         -1.2168e-01,  2.5894e-01,  9.5689e-02, -1.1439e-01, -4.1711e-01,\n",
       "          6.7122e-01, -3.5102e-02, -2.4058e-01, -3.9439e-01, -4.6817e-01,\n",
       "          7.5532e-02,  4.6120e-01, -5.1334e-01, -2.3204e-01, -3.5203e-01,\n",
       "         -4.7336e-01,  1.4070e-01,  5.7063e-01, -6.7928e-02,  1.2066e-01,\n",
       "         -1.2063e-01, -1.4190e-01, -8.8835e-02, -1.9721e-01, -2.8032e-01,\n",
       "         -2.7795e-01, -1.1303e-01,  6.7838e-01,  3.0960e-01,  1.3825e-01,\n",
       "          4.9660e-01, -6.7546e-01,  1.8165e-01,  4.6415e-01, -2.6523e-01,\n",
       "          1.1300e-02,  2.4267e-02, -8.6149e-02, -4.4508e-02,  1.3562e-01,\n",
       "         -6.8937e-02], grad_fn=<SelectBackward>)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0eb0f1c3e0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "l = torch.stack(l).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7244,  0.2185,  0.3596,  ..., -0.1213, -0.0361, -0.2045],\n",
       "         [-0.5270,  0.1055,  0.0569,  ..., -0.1807,  0.1780, -0.1330],\n",
       "         [-1.3411,  0.5691,  0.4539,  ..., -0.4401,  0.8559, -0.4931],\n",
       "         [-1.4175,  0.6699,  0.6074,  ..., -0.2983,  0.6935, -0.4998],\n",
       "         [-0.6032,  0.1023,  0.1544,  ..., -0.0445,  0.1356, -0.0689]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0697, -0.1123,  0.1430, -0.2184,  0.0787,  0.0187, -0.0501,\n",
      "           0.0788,  0.0626, -0.2354, -0.0062, -0.0107,  0.0975, -0.2407,\n",
      "          -0.0911, -0.0755,  0.2566, -0.0421,  0.0431,  0.0651, -0.2537,\n",
      "           0.1059, -0.1778, -0.2091,  0.0967,  0.1972,  0.0813,  0.0619,\n",
      "           0.1589, -0.0146, -0.0901, -0.2080, -0.1760,  0.0839,  0.1264,\n",
      "          -0.1739,  0.0644,  0.1006, -0.2319,  0.0192, -0.0259,  0.1832,\n",
      "           0.0511,  0.2910, -0.0415,  0.1478, -0.0356, -0.2326, -0.1257,\n",
      "           0.2045,  0.0965,  0.2925, -0.1515,  0.1365,  0.1837,  0.2794,\n",
      "           0.2009, -0.2155, -0.1598, -0.0579,  0.0311,  0.0285, -0.1294,\n",
      "           0.0448, -0.2758,  0.2094,  0.0631,  0.1532, -0.0012, -0.0005,\n",
      "          -0.0095,  0.0590,  0.1526, -0.1357,  0.2336,  0.0964, -0.0035,\n",
      "          -0.0552,  0.0781,  0.0447,  0.2604,  0.0741,  0.0430, -0.0198,\n",
      "           0.1427, -0.0479,  0.2146,  0.1687, -0.1059,  0.1806,  0.1822,\n",
      "          -0.0504,  0.0199, -0.2472,  0.0645, -0.0220,  0.1739,  0.1183,\n",
      "           0.0918, -0.0477, -0.1973,  0.2526, -0.0211,  0.0305, -0.1260,\n",
      "          -0.2160,  0.0219,  0.0212,  0.0665,  0.1794,  0.1200, -0.0806,\n",
      "          -0.0802,  0.3335, -0.1219,  0.0977,  0.2415, -0.0293,  0.0337,\n",
      "           0.0071,  0.0930,  0.0800, -0.3399,  0.0085, -0.0886, -0.2069,\n",
      "           0.1317, -0.0397, -0.1396, -0.1713, -0.2225,  0.1459,  0.0999,\n",
      "          -0.1567,  0.1651, -0.0735,  0.0120, -0.1911, -0.3176,  0.1946,\n",
      "           0.1245,  0.0628,  0.1634,  0.0316, -0.1078, -0.1270,  0.3338,\n",
      "           0.1010, -0.2695,  0.2411,  0.1303, -0.1989,  0.0277,  0.1395,\n",
      "          -0.0485, -0.2148, -0.1909,  0.1477,  0.1919,  0.1870, -0.1305,\n",
      "           0.1970, -0.0899,  0.1071, -0.0110,  0.2981, -0.0724, -0.0603,\n",
      "           0.1267,  0.1224,  0.1958, -0.0912,  0.1160, -0.1243,  0.1467,\n",
      "           0.1383, -0.0848,  0.1731, -0.0312,  0.1782,  0.0084,  0.0492,\n",
      "           0.0532, -0.0598, -0.1195, -0.0573,  0.0329, -0.1803, -0.1363,\n",
      "          -0.0026,  0.0324, -0.0515,  0.0482, -0.2354,  0.3300,  0.1945,\n",
      "           0.1479,  0.0097, -0.0168,  0.1339, -0.0707,  0.1672,  0.1813,\n",
      "           0.0189, -0.2109, -0.1336, -0.0618, -0.0783,  0.0023,  0.0569,\n",
      "          -0.1399, -0.0460,  0.0427, -0.0368,  0.1118, -0.0041, -0.0411,\n",
      "           0.1977,  0.2616,  0.1069, -0.2059,  0.1169,  0.0048, -0.0499,\n",
      "           0.0066, -0.1336, -0.1231,  0.0686,  0.0979,  0.0070, -0.1912,\n",
      "          -0.0098, -0.0326, -0.1323, -0.1245,  0.0431, -0.1931, -0.2584,\n",
      "          -0.1024, -0.0188,  0.0206, -0.1490,  0.1003, -0.2186,  0.1301,\n",
      "           0.3117,  0.2421, -0.0215,  0.2409, -0.0810,  0.1300,  0.1310,\n",
      "           0.0367, -0.0778,  0.0649, -0.0690]]], grad_fn=<StackBackward>), tensor([[[ 1.3705e-01, -2.1467e-01,  2.8164e-01, -5.2334e-01,  1.5697e-01,\n",
      "           3.9925e-02, -8.7045e-02,  1.4797e-01,  1.4191e-01, -4.2220e-01,\n",
      "          -1.3962e-02, -2.2135e-02,  2.3779e-01, -5.0428e-01, -1.9115e-01,\n",
      "          -1.6959e-01,  4.9748e-01, -1.0486e-01,  8.7587e-02,  1.1599e-01,\n",
      "          -5.1800e-01,  1.9981e-01, -3.4388e-01, -4.5372e-01,  1.6243e-01,\n",
      "           4.0351e-01,  1.4233e-01,  1.1066e-01,  3.7581e-01, -2.9207e-02,\n",
      "          -1.4654e-01, -5.1902e-01, -3.2729e-01,  1.8552e-01,  2.2284e-01,\n",
      "          -3.3298e-01,  1.3141e-01,  2.1704e-01, -4.1327e-01,  4.6333e-02,\n",
      "          -4.6416e-02,  2.6854e-01,  1.2815e-01,  7.1007e-01, -8.8994e-02,\n",
      "           3.2328e-01, -8.0111e-02, -5.3840e-01, -2.6420e-01,  3.9758e-01,\n",
      "           2.3219e-01,  6.2250e-01, -4.8257e-01,  2.9912e-01,  4.9488e-01,\n",
      "           5.5530e-01,  4.1226e-01, -4.3077e-01, -3.2327e-01, -1.1600e-01,\n",
      "           6.6690e-02,  7.7250e-02, -2.5606e-01,  1.0065e-01, -4.9506e-01,\n",
      "           3.8620e-01,  1.1978e-01,  3.6015e-01, -3.2811e-03, -9.8042e-04,\n",
      "          -1.5366e-02,  1.5605e-01,  2.7913e-01, -2.8369e-01,  4.7464e-01,\n",
      "           1.9712e-01, -7.7400e-03, -1.1151e-01,  2.2808e-01,  9.1637e-02,\n",
      "           5.5471e-01,  1.6481e-01,  8.1467e-02, -3.7228e-02,  3.2453e-01,\n",
      "          -1.0212e-01,  5.5919e-01,  3.3413e-01, -2.2190e-01,  4.0405e-01,\n",
      "           4.5628e-01, -9.5649e-02,  4.1421e-02, -5.1355e-01,  1.1635e-01,\n",
      "          -5.5801e-02,  3.9262e-01,  2.4334e-01,  1.5788e-01, -9.2824e-02,\n",
      "          -4.8437e-01,  5.8582e-01, -4.4213e-02,  5.9536e-02, -2.9184e-01,\n",
      "          -4.0008e-01,  5.0449e-02,  5.3070e-02,  1.4253e-01,  3.3068e-01,\n",
      "           2.3430e-01, -1.7059e-01, -1.6661e-01,  6.4531e-01, -2.0583e-01,\n",
      "           1.6509e-01,  6.2858e-01, -7.5003e-02,  6.2151e-02,  1.3704e-02,\n",
      "           2.0359e-01,  1.7635e-01, -5.8861e-01,  1.5076e-02, -1.6093e-01,\n",
      "          -4.1749e-01,  2.7827e-01, -8.9250e-02, -4.8482e-01, -3.3933e-01,\n",
      "          -4.8243e-01,  2.6960e-01,  1.7519e-01, -3.8369e-01,  3.1432e-01,\n",
      "          -1.3767e-01,  2.6849e-02, -4.4414e-01, -7.5423e-01,  3.7526e-01,\n",
      "           2.2902e-01,  1.2895e-01,  3.3620e-01,  5.1544e-02, -2.1621e-01,\n",
      "          -3.0349e-01,  7.1303e-01,  2.0320e-01, -5.5678e-01,  4.5535e-01,\n",
      "           2.9920e-01, -6.6623e-01,  6.9023e-02,  3.2614e-01, -1.3133e-01,\n",
      "          -4.3632e-01, -4.1978e-01,  3.7327e-01,  3.8766e-01,  3.6119e-01,\n",
      "          -2.2290e-01,  3.8445e-01, -1.8739e-01,  2.1822e-01, -2.2172e-02,\n",
      "           7.6456e-01, -1.3094e-01, -1.0807e-01,  2.7848e-01,  2.5003e-01,\n",
      "           6.0767e-01, -1.6035e-01,  2.4888e-01, -2.9601e-01,  3.5826e-01,\n",
      "           2.4019e-01, -1.4747e-01,  3.3502e-01, -6.1318e-02,  3.7358e-01,\n",
      "           1.7953e-02,  9.1680e-02,  8.9969e-02, -1.4715e-01, -2.3155e-01,\n",
      "          -1.0973e-01,  5.6700e-02, -3.9153e-01, -2.6609e-01, -6.8911e-03,\n",
      "           6.2510e-02, -1.1605e-01,  1.2220e-01, -5.5718e-01,  1.0247e+00,\n",
      "           4.2378e-01,  3.1638e-01,  2.1574e-02, -3.3914e-02,  2.6341e-01,\n",
      "          -1.5092e-01,  2.6825e-01,  3.1640e-01,  3.8489e-02, -4.8320e-01,\n",
      "          -2.6438e-01, -1.4058e-01, -1.4415e-01,  4.6488e-03,  1.2636e-01,\n",
      "          -3.5833e-01, -8.3767e-02,  7.8092e-02, -8.2882e-02,  2.7383e-01,\n",
      "          -7.6616e-03, -9.4525e-02,  5.2609e-01,  5.2254e-01,  2.1228e-01,\n",
      "          -5.4910e-01,  2.6804e-01,  1.1255e-02, -1.0283e-01,  1.3085e-02,\n",
      "          -2.8456e-01, -2.6899e-01,  1.3869e-01,  1.7517e-01,  1.4005e-02,\n",
      "          -3.7997e-01, -2.0732e-02, -6.0676e-02, -2.6629e-01, -2.2768e-01,\n",
      "           7.3640e-02, -3.8683e-01, -5.4954e-01, -1.9784e-01, -3.7394e-02,\n",
      "           4.7115e-02, -2.8612e-01,  1.8685e-01, -4.3453e-01,  2.7024e-01,\n",
      "           6.8968e-01,  7.8555e-01, -3.8859e-02,  4.3624e-01, -1.7830e-01,\n",
      "           2.3917e-01,  2.4608e-01,  8.0082e-02, -1.6671e-01,  1.3570e-01,\n",
      "          -1.3432e-01]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = hidden[-1].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3705e-01, -2.1467e-01,  2.8164e-01, -5.2334e-01,  1.5697e-01,\n",
       "         3.9925e-02, -8.7045e-02,  1.4797e-01,  1.4191e-01, -4.2220e-01,\n",
       "        -1.3962e-02, -2.2135e-02,  2.3779e-01, -5.0428e-01, -1.9115e-01,\n",
       "        -1.6959e-01,  4.9748e-01, -1.0486e-01,  8.7587e-02,  1.1599e-01,\n",
       "        -5.1800e-01,  1.9981e-01, -3.4388e-01, -4.5372e-01,  1.6243e-01,\n",
       "         4.0351e-01,  1.4233e-01,  1.1066e-01,  3.7581e-01, -2.9207e-02,\n",
       "        -1.4654e-01, -5.1902e-01, -3.2729e-01,  1.8552e-01,  2.2284e-01,\n",
       "        -3.3298e-01,  1.3141e-01,  2.1704e-01, -4.1327e-01,  4.6333e-02,\n",
       "        -4.6416e-02,  2.6854e-01,  1.2815e-01,  7.1007e-01, -8.8994e-02,\n",
       "         3.2328e-01, -8.0111e-02, -5.3840e-01, -2.6420e-01,  3.9758e-01,\n",
       "         2.3219e-01,  6.2250e-01, -4.8257e-01,  2.9912e-01,  4.9488e-01,\n",
       "         5.5530e-01,  4.1226e-01, -4.3077e-01, -3.2327e-01, -1.1600e-01,\n",
       "         6.6690e-02,  7.7250e-02, -2.5606e-01,  1.0065e-01, -4.9506e-01,\n",
       "         3.8620e-01,  1.1978e-01,  3.6015e-01, -3.2811e-03, -9.8042e-04,\n",
       "        -1.5366e-02,  1.5605e-01,  2.7913e-01, -2.8369e-01,  4.7464e-01,\n",
       "         1.9712e-01, -7.7400e-03, -1.1151e-01,  2.2808e-01,  9.1637e-02,\n",
       "         5.5471e-01,  1.6481e-01,  8.1467e-02, -3.7228e-02,  3.2453e-01,\n",
       "        -1.0212e-01,  5.5919e-01,  3.3413e-01, -2.2190e-01,  4.0405e-01,\n",
       "         4.5628e-01, -9.5649e-02,  4.1421e-02, -5.1355e-01,  1.1635e-01,\n",
       "        -5.5801e-02,  3.9262e-01,  2.4334e-01,  1.5788e-01, -9.2824e-02,\n",
       "        -4.8437e-01,  5.8582e-01, -4.4213e-02,  5.9536e-02, -2.9184e-01,\n",
       "        -4.0008e-01,  5.0449e-02,  5.3070e-02,  1.4253e-01,  3.3068e-01,\n",
       "         2.3430e-01, -1.7059e-01, -1.6661e-01,  6.4531e-01, -2.0583e-01,\n",
       "         1.6509e-01,  6.2858e-01, -7.5003e-02,  6.2151e-02,  1.3704e-02,\n",
       "         2.0359e-01,  1.7635e-01, -5.8861e-01,  1.5076e-02, -1.6093e-01,\n",
       "        -4.1749e-01,  2.7827e-01, -8.9250e-02, -4.8482e-01, -3.3933e-01,\n",
       "        -4.8243e-01,  2.6960e-01,  1.7519e-01, -3.8369e-01,  3.1432e-01,\n",
       "        -1.3767e-01,  2.6849e-02, -4.4414e-01, -7.5423e-01,  3.7526e-01,\n",
       "         2.2902e-01,  1.2895e-01,  3.3620e-01,  5.1544e-02, -2.1621e-01,\n",
       "        -3.0349e-01,  7.1303e-01,  2.0320e-01, -5.5678e-01,  4.5535e-01,\n",
       "         2.9920e-01, -6.6623e-01,  6.9023e-02,  3.2614e-01, -1.3133e-01,\n",
       "        -4.3632e-01, -4.1978e-01,  3.7327e-01,  3.8766e-01,  3.6119e-01,\n",
       "        -2.2290e-01,  3.8445e-01, -1.8739e-01,  2.1822e-01, -2.2172e-02,\n",
       "         7.6456e-01, -1.3094e-01, -1.0807e-01,  2.7848e-01,  2.5003e-01,\n",
       "         6.0767e-01, -1.6035e-01,  2.4888e-01, -2.9601e-01,  3.5826e-01,\n",
       "         2.4019e-01, -1.4747e-01,  3.3502e-01, -6.1318e-02,  3.7358e-01,\n",
       "         1.7953e-02,  9.1680e-02,  8.9969e-02, -1.4715e-01, -2.3155e-01,\n",
       "        -1.0973e-01,  5.6700e-02, -3.9153e-01, -2.6609e-01, -6.8911e-03,\n",
       "         6.2510e-02, -1.1605e-01,  1.2220e-01, -5.5718e-01,  1.0247e+00,\n",
       "         4.2378e-01,  3.1638e-01,  2.1574e-02, -3.3914e-02,  2.6341e-01,\n",
       "        -1.5092e-01,  2.6825e-01,  3.1640e-01,  3.8489e-02, -4.8320e-01,\n",
       "        -2.6438e-01, -1.4058e-01, -1.4415e-01,  4.6488e-03,  1.2636e-01,\n",
       "        -3.5833e-01, -8.3767e-02,  7.8092e-02, -8.2882e-02,  2.7383e-01,\n",
       "        -7.6616e-03, -9.4525e-02,  5.2609e-01,  5.2254e-01,  2.1228e-01,\n",
       "        -5.4910e-01,  2.6804e-01,  1.1255e-02, -1.0283e-01,  1.3085e-02,\n",
       "        -2.8456e-01, -2.6899e-01,  1.3869e-01,  1.7517e-01,  1.4005e-02,\n",
       "        -3.7997e-01, -2.0732e-02, -6.0676e-02, -2.6629e-01, -2.2768e-01,\n",
       "         7.3640e-02, -3.8683e-01, -5.4954e-01, -1.9784e-01, -3.7394e-02,\n",
       "         4.7115e-02, -2.8612e-01,  1.8685e-01, -4.3453e-01,  2.7024e-01,\n",
       "         6.8968e-01,  7.8555e-01, -3.8859e-02,  4.3624e-01, -1.7830e-01,\n",
       "         2.3917e-01,  2.4608e-01,  8.0082e-02, -1.6671e-01,  1.3570e-01,\n",
       "        -1.3432e-01], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0451,  0.1707, -0.1887, -0.0803, -0.3338,  0.0849,  0.0994,\n",
       "          -0.0092,  0.0209, -0.2359],\n",
       "         [ 0.0701,  0.2317, -0.1775, -0.0161, -0.2372,  0.0775,  0.0865,\n",
       "          -0.0051,  0.0095, -0.3846],\n",
       "         [ 0.1011,  0.2673, -0.2765, -0.0446, -0.2541,  0.1182,  0.1001,\n",
       "          -0.0210,  0.0391, -0.4430],\n",
       "         [ 0.0816,  0.3171, -0.3153, -0.0760, -0.3497,  0.1926,  0.0474,\n",
       "          -0.0311,  0.0793, -0.5933],\n",
       "         [ 0.0661,  0.3829, -0.2475, -0.0384, -0.2970,  0.2038,  0.0891,\n",
       "          -0.0173,  0.0254, -0.7366]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_embs, hidden = lstm(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0695,  0.4406, -0.2393,  0.4299, -0.3749,  0.7223, -0.1267,\n",
       "            0.1923, -0.2109,  0.0517]],\n",
       " \n",
       "         [[-0.1012,  0.2199,  0.1219, -0.0128, -0.2094, -0.0616,  0.0540,\n",
       "            0.1520,  0.0629,  0.0184]],\n",
       " \n",
       "         [[ 0.0740,  0.0130, -0.0857, -0.1324, -0.0572,  0.0302,  0.2230,\n",
       "            0.0934, -0.0153,  0.1301]],\n",
       " \n",
       "         [[ 0.0095, -0.0506,  0.0270, -0.0427,  0.1398,  0.1236,  0.0012,\n",
       "           -0.1071,  0.0447,  0.0012]],\n",
       " \n",
       "         [[ 0.0429,  0.1184,  0.2333, -0.0728, -0.0811,  0.2270,  0.0814,\n",
       "           -0.0912,  0.0040,  0.1718]],\n",
       " \n",
       "         [[ 0.1744,  0.0111, -0.1330, -0.1384, -0.0017,  0.0188,  0.0360,\n",
       "            0.1133, -0.0173, -0.1226]],\n",
       " \n",
       "         [[ 0.0173,  0.0233,  0.0372,  0.0795, -0.2491, -0.0192, -0.1484,\n",
       "           -0.1563,  0.2679, -0.0179]],\n",
       " \n",
       "         [[ 0.0739, -0.0769, -0.1860,  0.0529, -0.1925, -0.0917,  0.0945,\n",
       "           -0.1328,  0.1562,  0.1146]],\n",
       " \n",
       "         [[-0.1203, -0.1043,  0.2104, -0.0267,  0.0551, -0.1584,  0.1248,\n",
       "            0.1935, -0.2197,  0.0075]],\n",
       " \n",
       "         [[ 0.0293,  0.1679, -0.0138, -0.0610,  0.0184,  0.0475,  0.0631,\n",
       "           -0.1595,  0.2108,  0.0741]]], grad_fn=<StackBackward>),\n",
       " tensor([[[ 0.1069,  0.7045, -0.2606,  1.6681, -0.9271,  0.9159, -0.1331,\n",
       "            0.2985, -0.2851,  0.5930]],\n",
       " \n",
       "         [[-0.2573,  0.5152,  0.2877, -0.0216, -0.5066, -0.1295,  0.1275,\n",
       "            0.3213,  0.1486,  0.0433]],\n",
       " \n",
       "         [[ 0.1200,  0.0244, -0.1631, -0.2564, -0.1273,  0.0591,  0.6328,\n",
       "            0.1897, -0.0310,  0.2507]],\n",
       " \n",
       "         [[ 0.0178, -0.0935,  0.0730, -0.0812,  0.3808,  0.3131,  0.0022,\n",
       "           -0.2655,  0.0957,  0.0024]],\n",
       " \n",
       "         [[ 0.0897,  0.2719,  0.4212, -0.1668, -0.2014,  0.5010,  0.1871,\n",
       "           -0.2005,  0.0065,  0.3752]],\n",
       " \n",
       "         [[ 0.5000,  0.0258, -0.2175, -0.2987, -0.0035,  0.0503,  0.0845,\n",
       "            0.2723, -0.0311, -0.2266]],\n",
       " \n",
       "         [[ 0.0337,  0.0454,  0.0840,  0.1445, -0.3941, -0.0471, -0.2452,\n",
       "           -0.2772,  0.5442, -0.0478]],\n",
       " \n",
       "         [[ 0.1388, -0.1381, -0.3086,  0.0856, -0.4366, -0.1881,  0.1624,\n",
       "           -0.2658,  0.2575,  0.3389]],\n",
       " \n",
       "         [[-0.2440, -0.2061,  0.5515, -0.0497,  0.0962, -0.3341,  0.2910,\n",
       "            0.3901, -0.4476,  0.0137]],\n",
       " \n",
       "         [[ 0.0774,  0.5427, -0.0275, -0.0895,  0.0399,  0.0898,  0.1652,\n",
       "           -0.3351,  0.4567,  0.1477]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1622,  0.0380,  0.0270, -0.0840, -0.0621, -0.0490,  0.0329,  0.0838,\n",
       "         0.0326,  0.1116], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[:, -1, :][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.tensor([[2., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1622,  0.0380,  0.0270, -0.0840, -0.0621, -0.0490,  0.0329,  0.0838,\n",
       "          0.0326,  0.1116,  2.0000,  5.0000,  6.0000]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((walk_embs[:, -1, :], f), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only one dimension can be inferred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-367-a484231d13f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: only one dimension can be inferred"
     ]
    }
   ],
   "source": [
    "hn.view(-1, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(walk_embs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1932]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2824,  0.3015, -0.2668,  ...,  0.2538,  0.1729, -0.2005],\n",
       "        [ 0.3229,  0.2920, -0.2846,  ...,  0.2517,  0.1716, -0.1769],\n",
       "        [ 0.3446,  0.2820, -0.2939,  ...,  0.2483,  0.1650, -0.1690],\n",
       "        [ 0.3562,  0.2752, -0.2993,  ...,  0.2455,  0.1587, -0.1662],\n",
       "        [ 0.3624,  0.2713, -0.3025,  ...,  0.2434,  0.1541, -0.1650]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-66887fa826b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 0 with size 5"
     ]
    }
   ],
   "source": [
    "embs[len([3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 3) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 2.0531,  0.2745, -0.0366]]),\n",
       " tensor([[ 0.0588,  0.5562, -1.3053]]),\n",
       " tensor([[0.4513, 0.2132, 1.7349]]),\n",
       " tensor([[-0.0582,  1.3860,  0.5462]]),\n",
       " tensor([[-0.2682, -0.8007, -0.0899]])]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0531,  0.2745, -0.0366]],\n",
       "\n",
       "        [[ 0.0588,  0.5562, -1.3053]],\n",
       "\n",
       "        [[ 0.4513,  0.2132,  1.7349]],\n",
       "\n",
       "        [[-0.0582,  1.3860,  0.5462]],\n",
       "\n",
       "        [[-0.2682, -0.8007, -0.0899]]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {2, 5, 8, 11, 14},\n",
       "             2: {0, 1, 7, 8, 9, 14},\n",
       "             5: {0, 3, 7, 9, 12, 13},\n",
       "             8: {0, 2, 9, 13},\n",
       "             11: {0, 1, 10, 12, 13},\n",
       "             14: {0, 2, 3, 10, 12, 13},\n",
       "             1: {2, 3, 11, 13},\n",
       "             3: {1, 5, 6, 14},\n",
       "             13: {1, 4, 5, 6, 8, 10, 11, 12, 14},\n",
       "             7: {2, 4, 5},\n",
       "             9: {2, 5, 8},\n",
       "             6: {3, 12, 13},\n",
       "             4: {7, 12, 13},\n",
       "             12: {4, 5, 6, 11, 13, 14},\n",
       "             10: {11, 13, 14}})"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [[2, 7, 5, 9], [2, 7, 5, 9, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_emb = agent.embed_graph(p.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_emb = get_states_emb(paths, graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_emb = graph_emb[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_emb = next_emb.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-467-bccf75fad53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m q_network_inputs = torch.stack([\n\u001b[0;32m----> 2\u001b[0;31m torch.cat([[get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)] for next_vertex in {2, 5, 8}]\n\u001b[0m\u001b[1;32m      3\u001b[0m             )])\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "q_network_inputs = torch.stack([\n",
    "torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)] for next_vertex in {2, 5, 8}]\n",
    "            )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 512 and 256 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-488-5966dd4e06a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_states_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_vertex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_vertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-488-5966dd4e06a5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_states_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_vertex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_vertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 512 and 256 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "[torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)]) for next_vertex in {2, 5, 8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -1.0753e+00,  8.3016e-02,  9.0379e-01,\n",
      "         -4.5027e-01, -4.9623e-01, -3.3664e-01,  1.6372e-01,  7.8192e-01,\n",
      "         -6.3847e-01,  8.2804e-01, -7.6104e-01, -1.3896e+00, -1.6419e-01,\n",
      "          6.8150e-01, -1.0698e+00,  3.1216e-01, -5.0953e-01,  5.5165e-01,\n",
      "         -1.0343e-01, -3.3589e-02,  1.1074e+00, -9.2738e-01,  5.6787e-01,\n",
      "          9.7487e-01,  4.1884e-02, -2.8618e-01,  3.1022e-01,  1.7354e+00,\n",
      "         -2.1475e-01, -5.0389e-01,  1.1795e+00,  9.8472e-01, -6.9413e-01,\n",
      "         -8.1964e-01,  1.0435e+00,  9.9066e-01,  1.0890e+00,  3.0548e-01,\n",
      "          7.1718e-01,  2.9464e-01, -4.4148e-01,  4.2255e-02, -1.3382e-01,\n",
      "         -1.1807e+00,  8.5676e-01,  1.3887e-01,  1.0270e-01, -5.3805e-01,\n",
      "          8.6735e-02,  1.9163e-01,  1.1998e+00,  9.6794e-01, -5.3834e-01,\n",
      "         -1.0429e+00, -1.0141e-01, -1.2548e-01, -4.8603e-01, -2.8472e-01,\n",
      "          6.8429e-02, -4.5848e-01,  1.0943e+00, -3.3143e-02,  4.7038e-01,\n",
      "          1.7765e-01,  8.0808e-01, -9.2209e-01, -1.4332e-01,  8.2519e-02,\n",
      "         -4.9489e-01,  6.1177e-01,  1.2491e+00, -1.1702e+00,  3.1776e-01,\n",
      "         -1.0395e+00,  4.4867e-01, -9.2202e-02,  5.1838e-01,  1.6764e-01,\n",
      "          1.8922e-01, -8.0567e-01, -4.6363e-01, -1.3145e+00,  7.8315e-01,\n",
      "         -3.3179e-01,  7.9623e-01, -6.0810e-01, -9.0123e-01, -1.3840e-01,\n",
      "          9.1656e-01, -1.2249e+00,  2.8279e-01,  3.8269e-01,  1.3132e+00,\n",
      "          5.3049e-01,  2.9040e-01,  5.5860e-01, -1.4551e-01,  2.1765e-01,\n",
      "          1.5090e-01, -4.5296e-01,  6.2718e-01,  1.2221e+00, -6.4133e-01,\n",
      "          1.4760e+00, -3.4876e-01, -5.5059e-01,  5.1183e-01, -7.2533e-01,\n",
      "          4.9328e-01, -7.2075e-01, -2.4347e-01, -1.4319e-02,  4.9916e-01,\n",
      "         -1.8595e-01,  7.6508e-01, -2.9328e-01,  8.8973e-02,  1.4872e-01,\n",
      "          1.9510e-01,  1.5322e-01,  6.1525e-01,  1.7307e+00, -5.1634e-01,\n",
      "         -2.3653e-01,  1.1662e-01, -6.0022e-01,  4.3567e-01,  3.6696e-01,\n",
      "         -9.3369e-01,  9.2049e-01,  7.9412e-01,  3.8536e-01,  5.2863e-01,\n",
      "         -1.2859e+00, -9.0379e-03, -3.4594e-02, -1.0326e+00, -2.6977e-01,\n",
      "          1.0193e+00, -4.6771e-01,  4.8758e-01,  7.4471e-01,  1.1128e+00,\n",
      "         -8.8620e-01,  2.0674e-01,  3.3264e-01, -7.7463e-01, -3.9493e-01,\n",
      "         -7.0258e-01,  2.3214e-01,  1.6084e+00, -1.6532e-02,  3.2143e-01,\n",
      "          1.7155e-02, -2.3366e-01, -5.0310e-01,  4.2611e-01,  3.2076e-01,\n",
      "          4.3838e-01,  1.4198e+00,  1.0128e+00,  5.4539e-01, -2.0881e-01,\n",
      "          7.8958e-01, -7.0304e-01, -1.0269e+00, -9.4981e-01,  8.2906e-01,\n",
      "          3.4686e-01, -1.0571e+00, -1.7160e-01, -4.9093e-01,  1.4063e+00,\n",
      "          6.8785e-02, -1.8559e-01, -2.6422e-01, -6.0021e-01, -3.8235e-01,\n",
      "         -2.9070e-01, -7.9425e-01, -6.0181e-01,  3.4000e-01,  8.0562e-02,\n",
      "         -2.8659e-01,  3.7100e-01,  5.8875e-01, -1.0452e+00, -1.2472e+00,\n",
      "          9.1564e-02, -2.0688e-01, -1.5108e-01,  8.5732e-01,  7.6452e-01,\n",
      "         -4.2799e-01,  6.6479e-01, -7.8041e-02, -7.8257e-01, -1.2416e-02,\n",
      "         -5.5353e-01,  2.4315e-01, -8.6182e-02,  7.9388e-01, -5.1352e-01,\n",
      "         -1.2207e+00,  9.7136e-02, -7.8171e-01,  3.5285e-01,  6.7947e-01,\n",
      "          5.3724e-01, -1.2459e+00,  1.8495e-01,  7.0043e-01, -3.5441e-02,\n",
      "          5.1074e-01, -1.1365e+00, -3.4992e-01, -1.0458e+00, -2.7660e-01,\n",
      "          1.1339e+00, -3.7067e-01,  5.1654e-01, -2.8271e-01, -8.9785e-01,\n",
      "         -3.5456e-01,  2.9707e-01, -7.4910e-01,  2.5008e-01, -5.8589e-01,\n",
      "          4.4527e-01, -5.1481e-01, -2.7125e-01,  3.2400e-01, -3.5406e-01,\n",
      "         -9.1517e-01, -1.2014e+00, -1.2017e-01,  3.1524e-01, -1.5097e-01,\n",
      "          8.9411e-01, -8.5860e-01, -2.8956e-01, -5.7616e-01, -9.2444e-01,\n",
      "         -4.8267e-01,  1.2769e-01,  1.8004e+00,  3.1816e-01, -2.1505e-01,\n",
      "          4.8580e-01,  8.3232e-01, -8.3170e-03,  1.3702e-01, -1.4425e-01,\n",
      "          7.6971e-01,  9.9928e-01,  1.7671e-01]], grad_fn=<CatBackward>)\n",
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -9.4111e-01,  6.9399e-02,  7.8901e-01,\n",
      "         -9.0577e-01, -7.8137e-01, -3.2307e-01,  1.3018e-01,  1.1046e+00,\n",
      "         -5.6568e-01,  1.0193e+00, -4.5313e-01, -1.1985e+00, -5.1469e-01,\n",
      "          5.2364e-01, -1.2426e+00,  4.5910e-01, -5.6564e-01,  8.7383e-01,\n",
      "          3.9400e-02,  6.3916e-02,  1.2248e+00, -1.2303e+00,  8.1613e-01,\n",
      "          1.0470e+00, -1.8461e-01, -5.0045e-01,  5.9847e-01,  2.0177e+00,\n",
      "         -3.5044e-01, -5.0608e-01,  1.0964e+00,  1.0660e+00, -1.1168e+00,\n",
      "         -9.0968e-01,  1.2758e+00,  1.1305e+00,  1.1310e+00,  7.3110e-01,\n",
      "          8.7305e-01,  3.6757e-01, -5.5817e-01,  2.1573e-01, -1.2241e-01,\n",
      "         -1.4266e+00,  9.8910e-01,  2.4861e-01,  5.1556e-01, -4.8574e-01,\n",
      "          2.8105e-01,  2.1997e-01,  1.6452e+00,  8.5567e-01, -5.2171e-01,\n",
      "         -1.1339e+00, -1.8351e-01,  9.3438e-02, -7.6569e-01, -1.9778e-01,\n",
      "          2.4157e-01, -4.6584e-01,  1.4769e+00,  8.2124e-02,  9.2741e-01,\n",
      "          5.0643e-02,  1.1705e+00, -9.5360e-01, -2.5615e-01, -3.8089e-03,\n",
      "         -3.4076e-01,  6.5082e-01,  1.0418e+00, -1.2011e+00,  3.5959e-01,\n",
      "         -1.0397e+00,  5.0469e-01, -2.8869e-01,  5.6988e-01,  4.1276e-02,\n",
      "          2.7797e-01, -8.5169e-01, -2.8200e-01, -1.1773e+00,  8.1556e-01,\n",
      "         -2.8439e-01,  6.8981e-01, -9.3031e-01, -8.0251e-01, -1.8828e-01,\n",
      "          1.0650e+00, -1.3325e+00,  2.6191e-01,  3.2370e-01,  1.0845e+00,\n",
      "          5.6503e-01,  4.0922e-01,  2.0729e-01, -1.8715e-01,  1.9647e-01,\n",
      "         -1.2909e-01, -7.4647e-01,  9.1441e-01,  1.1838e+00, -3.9645e-01,\n",
      "          1.6914e+00, -3.5210e-01, -3.4453e-01,  5.8520e-01, -4.0602e-01,\n",
      "          7.7554e-01, -1.0866e+00, -5.0612e-02, -9.7199e-02,  5.3288e-01,\n",
      "         -6.1032e-02,  1.0681e+00, -5.9042e-01,  1.7983e-01,  3.0896e-01,\n",
      "          1.5855e-01,  2.0271e-01,  3.5542e-01,  1.9156e+00, -5.0487e-01,\n",
      "          2.1560e-02, -1.8025e-01, -6.7815e-01,  6.8125e-01,  2.6780e-01,\n",
      "         -1.2983e+00,  7.2077e-01,  1.3293e+00,  2.5311e-01,  5.2494e-01,\n",
      "         -1.4525e+00, -2.7671e-01, -4.8103e-01, -9.9362e-01, -1.1428e-01,\n",
      "          1.2033e+00, -4.0554e-01,  7.4388e-01,  9.8874e-01,  1.0805e+00,\n",
      "         -8.7663e-01,  1.5076e-01,  3.3455e-01, -9.4930e-01, -3.7420e-01,\n",
      "         -5.4317e-01,  1.8055e-01,  1.4846e+00, -1.2499e-02,  3.6847e-01,\n",
      "          1.7667e-01, -6.0996e-01, -6.3694e-01,  3.8995e-01,  1.7439e-01,\n",
      "          7.8642e-01,  1.5289e+00,  1.1468e+00,  1.0751e+00,  1.1781e-01,\n",
      "          1.2791e+00, -6.5682e-01, -1.3189e+00, -1.0855e+00,  8.6579e-01,\n",
      "          2.6453e-01, -1.0408e+00, -4.6901e-01, -4.8194e-01,  1.4052e+00,\n",
      "          1.0259e-01, -3.1057e-01, -5.7743e-01, -6.0951e-01, -3.3916e-01,\n",
      "         -1.9673e-01, -9.1445e-01, -8.0649e-01,  3.4176e-02, -4.9650e-02,\n",
      "         -2.1848e-01,  7.2256e-01,  7.8479e-01, -1.1299e+00, -1.1847e+00,\n",
      "         -2.4169e-02, -1.1446e-01, -3.6775e-01,  8.2433e-01,  8.8071e-01,\n",
      "         -3.4107e-01,  6.4769e-01, -3.0454e-01, -1.0259e+00,  1.5975e-01,\n",
      "         -6.2286e-01,  1.0777e-01, -2.5937e-01,  9.9342e-01, -4.5901e-01,\n",
      "         -1.2876e+00, -1.8518e-01, -5.1734e-01,  5.3465e-01,  5.8413e-01,\n",
      "          4.3348e-01, -1.2576e+00,  1.0642e-01,  1.0428e+00,  1.3208e-01,\n",
      "          7.6130e-01, -1.2414e+00, -8.5417e-02, -8.8439e-01, -4.0116e-01,\n",
      "          1.0803e+00, -2.0238e-01,  7.1424e-01, -1.5055e-01, -9.1289e-01,\n",
      "         -4.7191e-01,  7.2196e-01, -8.4662e-01,  1.9771e-01, -6.4393e-01,\n",
      "          4.7057e-01, -4.0980e-01,  5.8424e-02,  3.8889e-01, -3.4223e-01,\n",
      "         -1.1852e+00, -1.3812e+00,  3.4931e-01,  4.7213e-01, -3.6945e-01,\n",
      "          7.5271e-01, -1.0213e+00, -1.0307e-01, -9.1145e-01, -8.1439e-01,\n",
      "         -5.1228e-01,  3.0782e-01,  2.2749e+00,  2.7364e-01, -4.8049e-02,\n",
      "          9.8131e-01,  1.0277e+00,  2.9348e-01,  2.0049e-01, -4.0895e-02,\n",
      "          1.0486e+00,  9.7127e-01, -3.2769e-02]], grad_fn=<CatBackward>)\n",
      "tensor([[-9.7202e-01,  7.7923e-02,  7.6080e-01, -5.0376e-01, -5.6276e-01,\n",
      "         -2.8771e-01,  1.6087e-01,  7.3920e-01, -5.1050e-01,  7.5375e-01,\n",
      "         -5.5776e-01, -1.2383e+00, -2.9269e-01,  4.7613e-01, -9.8817e-01,\n",
      "          3.4015e-01, -4.5546e-01,  6.2430e-01, -6.5620e-02,  5.9016e-02,\n",
      "          1.0267e+00, -9.5899e-01,  6.1672e-01,  8.1769e-01, -7.9431e-02,\n",
      "         -3.2172e-01,  4.2354e-01,  1.6413e+00, -2.1666e-01, -4.8842e-01,\n",
      "          1.0008e+00,  9.2398e-01, -8.2213e-01, -7.6344e-01,  1.0147e+00,\n",
      "          9.6081e-01,  9.8737e-01,  4.3086e-01,  6.9872e-01,  3.2411e-01,\n",
      "         -4.8209e-01,  1.2577e-01, -1.0857e-01, -1.1443e+00,  8.4729e-01,\n",
      "          1.1149e-01,  2.5156e-01, -4.5330e-01,  1.2396e-01,  1.6943e-01,\n",
      "          1.2370e+00,  8.1178e-01, -4.2973e-01, -9.9228e-01, -1.7468e-01,\n",
      "         -9.6961e-02, -5.9296e-01, -2.4168e-01,  5.2235e-02, -4.4475e-01,\n",
      "          1.1245e+00, -1.8958e-02,  5.4813e-01,  1.2968e-01,  8.9272e-01,\n",
      "         -9.2226e-01, -1.6239e-01,  7.3994e-02, -3.6789e-01,  5.4102e-01,\n",
      "          1.0962e+00, -1.1678e+00,  2.9284e-01, -9.6934e-01,  4.1976e-01,\n",
      "         -1.3186e-01,  4.4399e-01,  1.3548e-01,  2.1622e-01, -7.2200e-01,\n",
      "         -3.9182e-01, -1.1665e+00,  7.1430e-01, -3.0106e-01,  6.9785e-01,\n",
      "         -6.5732e-01, -8.0638e-01, -1.2971e-01,  8.5984e-01, -1.0532e+00,\n",
      "          3.1857e-01,  3.5329e-01,  1.1181e+00,  4.6177e-01,  3.3599e-01,\n",
      "          3.3218e-01, -1.3950e-01,  1.4599e-01,  1.8112e-02, -4.7243e-01,\n",
      "          6.0966e-01,  1.1135e+00, -4.6608e-01,  1.4578e+00, -3.4160e-01,\n",
      "         -4.0551e-01,  4.8959e-01, -5.8518e-01,  5.1341e-01, -7.7428e-01,\n",
      "         -2.1400e-01, -5.6158e-02,  5.2042e-01, -1.1932e-01,  7.3646e-01,\n",
      "         -3.3599e-01,  9.0611e-02,  1.6735e-01,  1.3029e-01,  2.0930e-01,\n",
      "          4.8294e-01,  1.6467e+00, -5.1235e-01, -1.5108e-01, -2.6250e-02,\n",
      "         -5.9707e-01,  4.3820e-01,  2.6570e-01, -1.0202e+00,  7.1712e-01,\n",
      "          8.6380e-01,  3.0387e-01,  5.3093e-01, -1.1584e+00, -1.6408e-01,\n",
      "         -1.9432e-01, -9.2235e-01, -2.1086e-01,  9.7186e-01, -3.6606e-01,\n",
      "          5.5071e-01,  6.9117e-01,  9.7162e-01, -7.7106e-01,  1.3133e-01,\n",
      "          2.5406e-01, -7.4757e-01, -3.1572e-01, -5.5822e-01,  2.1497e-01,\n",
      "          1.4420e+00,  9.0430e-03,  3.7610e-01,  6.6689e-02, -3.1342e-01,\n",
      "         -4.6025e-01,  3.3748e-01,  1.6965e-01,  4.9672e-01,  1.3146e+00,\n",
      "          9.3529e-01,  6.8182e-01, -8.1613e-02,  8.4600e-01, -6.1854e-01,\n",
      "         -1.0197e+00, -9.4784e-01,  7.4145e-01,  3.5331e-01, -9.1836e-01,\n",
      "         -2.4170e-01, -4.6727e-01,  1.2478e+00,  9.2040e-02, -2.1468e-01,\n",
      "         -4.0311e-01, -5.8545e-01, -3.1131e-01, -2.8146e-01, -7.0161e-01,\n",
      "         -5.8223e-01,  1.9525e-01,  8.9032e-03, -2.1040e-01,  4.6633e-01,\n",
      "          6.3265e-01, -9.3115e-01, -1.1178e+00,  6.2612e-02, -8.1244e-02,\n",
      "         -1.5500e-01,  7.3996e-01,  6.8420e-01, -3.3598e-01,  5.3351e-01,\n",
      "         -1.3851e-01, -8.6270e-01,  7.9100e-02, -4.7320e-01,  1.3290e-01,\n",
      "         -1.4581e-01,  8.1337e-01, -4.0062e-01, -1.0862e+00, -5.2844e-04,\n",
      "         -6.0425e-01,  4.0830e-01,  5.6152e-01,  4.4234e-01, -1.0699e+00,\n",
      "          1.8404e-01,  7.3991e-01, -7.0996e-03,  4.9090e-01, -1.0507e+00,\n",
      "         -3.2077e-01, -9.2562e-01, -2.6521e-01,  1.0545e+00, -2.3061e-01,\n",
      "          5.3555e-01, -2.3286e-01, -8.5181e-01, -3.8874e-01,  3.9770e-01,\n",
      "         -6.9676e-01,  2.0092e-01, -4.8628e-01,  3.7434e-01, -4.3542e-01,\n",
      "         -1.2546e-01,  3.5577e-01, -3.2655e-01, -9.5951e-01, -1.1329e+00,\n",
      "          8.8865e-02,  3.3210e-01, -1.3369e-01,  7.4010e-01, -7.5352e-01,\n",
      "         -2.0687e-01, -6.5185e-01, -7.6853e-01, -5.0649e-01,  2.1735e-01,\n",
      "          1.7365e+00,  2.5531e-01, -2.0884e-01,  6.4170e-01,  8.5400e-01,\n",
      "          6.2953e-02,  1.8568e-01, -7.2704e-02,  7.9698e-01,  8.8072e-01,\n",
      "          1.1057e-01, -8.6766e-01,  8.7071e-02,  4.9569e-01, -1.7508e-01,\n",
      "         -3.2535e-01, -2.2563e-01,  2.9863e-01,  4.2876e-01, -4.0390e-01,\n",
      "          6.1847e-01, -3.4526e-01, -9.8930e-01, -2.2459e-01,  4.9828e-01,\n",
      "         -4.9236e-01,  1.9834e-02, -3.4551e-01,  5.2496e-01,  1.0111e-01,\n",
      "          1.1790e-01,  5.4485e-01, -7.4945e-01,  4.8833e-01,  5.9144e-01,\n",
      "          1.0262e-01, -6.5249e-02,  2.3441e-01,  1.1642e+00, -2.8038e-01,\n",
      "         -4.3459e-01,  7.2296e-01,  5.5768e-01, -6.1281e-01, -3.4998e-01,\n",
      "          8.9754e-01,  8.8741e-01,  8.6301e-01,  3.0240e-01,  5.1829e-01,\n",
      "          4.2088e-01, -4.3697e-01,  2.3697e-01, -1.2637e-01, -9.0174e-01,\n",
      "          6.4830e-01,  1.0744e-01,  7.1759e-02, -3.5314e-01,  1.9650e-01,\n",
      "          9.9194e-02,  7.1675e-01,  7.8747e-01, -3.1223e-01, -9.4662e-01,\n",
      "         -1.6618e-01, -2.4282e-01, -5.5800e-01, -2.5800e-01, -1.8025e-01,\n",
      "         -3.4747e-01,  8.4173e-01, -1.4170e-01,  4.3352e-01,  1.0599e-01,\n",
      "          5.2026e-01, -7.3729e-01, -2.3291e-01,  5.9857e-02, -2.6228e-01,\n",
      "          5.8144e-01,  9.7909e-01, -8.9637e-01,  3.6313e-01, -8.9137e-01,\n",
      "          3.9950e-01, -5.8908e-02,  2.9305e-01,  4.1540e-01, -2.5688e-02,\n",
      "         -5.2271e-01, -3.4876e-01, -9.6374e-01,  5.3753e-01, -4.2470e-01,\n",
      "          6.4716e-01, -4.6589e-01, -6.4473e-01, -1.4466e-01,  5.8131e-01,\n",
      "         -7.6037e-01,  1.3396e-01,  4.4783e-01,  9.6386e-01,  1.0823e-01,\n",
      "          1.6662e-01,  1.5428e-01, -1.5904e-01, -1.0163e-02,  1.3297e-01,\n",
      "         -2.2799e-01,  5.8355e-01,  7.4498e-01, -4.6634e-01,  1.1295e+00,\n",
      "         -4.4979e-01, -4.3552e-01,  3.9911e-01, -4.8334e-01,  5.0947e-01,\n",
      "         -5.3179e-01, -4.1284e-01, -8.9860e-02,  4.1641e-01, -3.0963e-01,\n",
      "          4.9797e-01, -3.3748e-01, -7.1963e-02,  1.9341e-01, -2.5251e-02,\n",
      "          3.3347e-01,  3.8591e-01,  1.0520e+00, -5.1168e-01,  4.0803e-02,\n",
      "          3.5645e-02, -3.1520e-01,  4.8232e-01,  2.3563e-01, -7.0435e-01,\n",
      "          6.8374e-01,  6.1458e-01,  4.4303e-01,  3.3166e-01, -7.2692e-01,\n",
      "         -1.1326e-01, -3.2580e-02, -6.4462e-01, -1.4722e-01,  6.7067e-01,\n",
      "         -2.4441e-01,  3.8381e-01,  4.6535e-01,  6.6858e-01, -6.6610e-01,\n",
      "          1.1394e-03,  9.6081e-02, -5.1337e-01, -3.6254e-01, -3.0018e-01,\n",
      "          1.5029e-01,  1.0267e+00,  4.1179e-02,  1.3933e-01,  1.9377e-01,\n",
      "         -2.1900e-01, -1.2536e-01,  1.2495e-01,  1.7075e-01,  3.2936e-01,\n",
      "          1.0592e+00,  7.9173e-01,  4.9684e-01, -2.1633e-01,  4.6958e-01,\n",
      "         -3.6272e-01, -6.1581e-01, -9.1005e-01,  5.6189e-01,  3.3400e-01,\n",
      "         -8.1927e-01, -3.3664e-02, -4.6623e-01,  1.1371e+00,  1.8723e-01,\n",
      "         -9.6725e-02, -1.2178e-01, -4.0942e-01, -2.4027e-01, -2.1731e-01,\n",
      "         -6.4868e-01, -4.1631e-01,  1.0331e-01,  1.2696e-01, -2.2023e-02,\n",
      "          2.3283e-01,  4.7913e-01, -7.9998e-01, -9.7749e-01,  1.4370e-01,\n",
      "         -2.7050e-01, -7.5745e-02,  5.3576e-01,  6.2689e-01, -3.3897e-01,\n",
      "          2.5598e-01, -1.2110e-01, -7.5497e-01,  4.3089e-02, -2.8001e-01,\n",
      "          8.6921e-02,  9.2621e-04,  5.9893e-01, -4.8494e-01, -6.6518e-01,\n",
      "         -2.8246e-02, -4.1001e-01,  3.6064e-01,  4.0148e-01,  3.7140e-01,\n",
      "         -8.5684e-01,  2.9886e-02,  5.0380e-01,  1.9309e-02,  2.4614e-01,\n",
      "         -8.0436e-01, -3.2657e-01, -7.2397e-01, -1.3787e-01,  5.4970e-01,\n",
      "         -2.1963e-01,  3.9818e-01, -1.7209e-01, -6.1456e-01, -2.9881e-01,\n",
      "          1.4443e-01, -6.8316e-01,  3.3515e-01, -3.2053e-01,  1.1038e-01,\n",
      "         -3.3141e-01, -1.0701e-01,  3.1507e-01, -1.8996e-01, -8.8835e-01,\n",
      "         -8.8940e-01, -5.5231e-02,  1.6904e-01, -1.2590e-01,  6.5735e-01,\n",
      "         -4.9049e-01, -1.9381e-01, -4.2452e-01, -8.3008e-01, -3.1223e-01,\n",
      "         -5.0811e-03,  1.0964e+00,  3.1523e-02, -4.0724e-01,  3.7294e-01,\n",
      "          6.7833e-01,  9.6920e-02,  1.2412e-01, -6.5691e-02,  4.9713e-01,\n",
      "          4.9584e-01,  1.7616e-01, -1.1723e+00,  2.3573e-02,  1.1856e+00,\n",
      "         -8.5422e-01, -8.9325e-01, -3.3991e-01, -7.2060e-02,  1.0866e+00,\n",
      "         -6.7398e-01,  9.3840e-01, -9.8908e-01, -1.6452e+00, -2.6995e-01,\n",
      "          4.8602e-01, -1.7519e+00,  7.9274e-01, -6.0133e-01,  7.2332e-01,\n",
      "         -3.9212e-01, -7.7101e-02,  1.7570e+00, -1.1990e+00,  7.9673e-01,\n",
      "          1.1428e+00, -3.1973e-01, -6.0894e-01,  6.6357e-01,  2.2914e+00,\n",
      "         -1.3100e-01, -5.6774e-01,  1.5216e+00,  1.5095e+00, -1.1044e+00,\n",
      "         -1.3798e+00,  1.1786e+00,  1.0798e+00,  1.1484e+00,  4.7308e-01,\n",
      "          9.6668e-01,  9.4255e-02, -5.0975e-01, -1.4433e-01, -8.9043e-02,\n",
      "         -1.4640e+00,  1.1395e+00,  1.1900e-01,  4.2808e-01, -5.6041e-01,\n",
      "         -1.8714e-02,  2.2924e-01,  1.8924e+00,  9.0358e-01, -5.9261e-01,\n",
      "         -1.0587e+00, -1.5071e-01,  8.3437e-02, -5.5269e-01, -2.6167e-01,\n",
      "          4.3624e-01, -6.0987e-01,  1.4460e+00,  9.9812e-02,  5.6888e-01,\n",
      "          2.2712e-01,  1.4274e+00, -1.2108e+00, -5.2010e-02,  1.5954e-01,\n",
      "         -6.2900e-01,  5.1123e-01,  1.3757e+00, -1.6354e+00,  1.9243e-01,\n",
      "         -1.0646e+00,  5.2548e-01, -1.5116e-01,  6.7149e-01, -2.0972e-01,\n",
      "          5.4147e-01, -1.0220e+00, -5.1922e-01, -1.5373e+00,  1.0401e+00,\n",
      "         -1.9116e-01,  8.4503e-01, -8.0682e-01, -1.0220e+00, -7.4441e-02,\n",
      "          1.2291e+00, -1.4517e+00,  6.3101e-01,  1.4829e-01,  1.4019e+00,\n",
      "          9.2249e-01,  5.7516e-01,  7.4769e-01, -1.4557e-01,  4.0282e-01,\n",
      "         -1.0100e-01, -7.7795e-01,  5.9548e-01,  1.7058e+00, -5.4068e-01,\n",
      "          1.9184e+00, -2.5282e-01, -4.7041e-01,  6.9842e-01, -8.8754e-01,\n",
      "          4.8333e-01, -1.0072e+00,  4.7502e-02,  6.0545e-02,  7.0749e-01,\n",
      "          1.9272e-01,  9.4856e-01, -2.3628e-01,  3.4345e-01,  4.5628e-02,\n",
      "          4.2474e-01,  4.7216e-02,  6.9181e-01,  2.5453e+00, -4.6087e-01,\n",
      "         -5.2527e-01, -2.8796e-02, -1.0114e+00,  2.5396e-01,  3.5185e-01,\n",
      "         -1.3896e+00,  8.2981e-01,  1.1502e+00,  1.4859e-01,  9.3504e-01,\n",
      "         -1.8107e+00, -2.2527e-01, -3.2658e-01, -1.3782e+00, -3.4930e-01,\n",
      "          1.4003e+00, -5.9557e-01,  7.2037e-01,  9.9102e-01,  1.4465e+00,\n",
      "         -1.0324e+00,  4.0348e-01,  4.7708e-01, -1.0521e+00, -2.3282e-01,\n",
      "         -1.0784e+00,  3.2205e-01,  2.2032e+00, -1.0120e-01,  7.1030e-01,\n",
      "         -1.4256e-01, -3.2018e-01, -9.7706e-01,  7.1271e-01,  2.5441e-01,\n",
      "          6.9973e-01,  1.6312e+00,  1.1465e+00,  8.7034e-01,  1.7938e-02,\n",
      "          1.3458e+00, -1.0666e+00, -1.5576e+00, -9.7041e-01,  1.0304e+00,\n",
      "          4.1473e-01, -1.1094e+00, -5.1066e-01, -5.4286e-01,  1.4525e+00,\n",
      "         -7.2574e-02, -3.4583e-01, -6.8513e-01, -8.7917e-01, -4.9479e-01,\n",
      "         -4.3796e-01, -7.8697e-01, -7.2398e-01,  4.4900e-01, -8.7873e-02,\n",
      "         -4.6648e-01,  7.2095e-01,  8.5806e-01, -1.1936e+00, -1.2559e+00,\n",
      "          3.6040e-02,  2.2550e-01, -2.0109e-01,  1.0767e+00,  7.1531e-01,\n",
      "         -3.8859e-01,  9.7796e-01, -4.8342e-02, -9.1806e-01,  5.1591e-02,\n",
      "         -7.7440e-01,  2.8365e-01, -3.1376e-01,  1.1173e+00, -2.5497e-01,\n",
      "         -1.7492e+00,  1.0033e-01, -1.0308e+00,  3.7886e-01,  8.7885e-01,\n",
      "          6.4588e-01, -1.4428e+00,  3.8143e-01,  1.0342e+00, -1.0301e-01,\n",
      "          8.0322e-01, -1.4690e+00, -3.6205e-01, -1.3921e+00, -4.9494e-01,\n",
      "          1.9240e+00, -3.0726e-01,  6.9311e-01, -3.6334e-01, -1.2516e+00,\n",
      "         -4.9807e-01,  6.8272e-01, -6.8015e-01, -4.8013e-02, -7.5551e-01,\n",
      "          7.3808e-01, -6.6118e-01, -2.5353e-01,  4.0849e-01, -4.5224e-01,\n",
      "         -1.0734e+00, -1.5369e+00,  1.3045e-01,  5.1338e-01, -5.1645e-02,\n",
      "          8.8677e-01, -1.1773e+00, -3.3693e-01, -9.1351e-01, -7.5913e-01,\n",
      "         -7.6862e-01,  5.5482e-01,  2.5532e+00,  6.4212e-01,  4.5936e-02,\n",
      "          9.3556e-01,  1.0732e+00, -6.5674e-02,  3.2637e-01, -1.7841e-01,\n",
      "          1.2111e+00,  1.4870e+00,  6.6005e-02]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "for next_vertex in {2, 5, 8}:\n",
    "    print(torch.cat([get_states_emb([path], graph_emb), graph_emb[next_vertex].unsqueeze(0)], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9720,  0.0779,  0.7608,  ...,  0.4971,  0.4958,  0.1762],\n",
       "        [-0.9927,  0.0789,  0.7894,  ...,  0.7697,  0.9993,  0.1767]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-505-2cee602f4b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaths_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_emb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "[torch.cat([get_states_emb(paths, graph_emb), graph_emb[next_vertex].unsqueeze(0)]) for next_vertex in {2, 5, 8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_states_emb([[1,2,3]], graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for i in x:\n",
    "    print(torch.cat([i, next_emb]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_embs = get_states_emb(paths, graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = agent.value(paths_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1743],\n",
       "        [-0.2147]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.9720,  0.0779,  0.7608,  ...,  0.7697,  0.9993,  0.1767],\n",
      "        [-0.9720,  0.0779,  0.7608,  ...,  1.0486,  0.9713, -0.0328],\n",
      "        [-0.9720,  0.0779,  0.7608,  ...,  1.2111,  1.4870,  0.0660]],\n",
      "       grad_fn=<StackBackward>), tensor([[-0.9927,  0.0789,  0.7894,  ...,  0.9108,  0.9007,  0.0928],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  0.4971,  0.4958,  0.1762],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  1.0486,  0.9713, -0.0328],\n",
      "        [-0.9927,  0.0789,  0.7894,  ...,  1.2688,  1.7698,  0.0705]],\n",
      "       grad_fn=<StackBackward>)]\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "for i, path in enumerate(paths):\n",
    "    next_embs = []\n",
    "    for next_vertex in p.edges[path[-1]]:\n",
    "        next_vertex_emb = graph_emb[next_vertex]\n",
    "        next_embs.append(torch.cat([paths_embs[i], next_vertex_emb]))\n",
    "    states.append(torch.stack(next_embs))\n",
    "print(states)\n",
    "predicts = []\n",
    "for i in states:\n",
    "    predicts.append(sm(log_reg(i)).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3215, 0.3476, 0.3309], grad_fn=<ViewBackward>),\n",
       " tensor([0.2441, 0.2059, 0.2333, 0.3167], grad_fn=<ViewBackward>)]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-553-07342f11f5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'states'"
     ]
    }
   ],
   "source": [
    "torch.states.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {2, 5, 8, 11, 14},\n",
       "             2: {0, 1, 7, 8, 9, 14},\n",
       "             5: {0, 3, 7, 9, 12, 13},\n",
       "             8: {0, 2, 9, 13},\n",
       "             11: {0, 1, 10, 12, 13},\n",
       "             14: {0, 2, 3, 10, 12, 13},\n",
       "             1: {2, 3, 11, 13},\n",
       "             3: {1, 5, 6, 14},\n",
       "             13: {1, 4, 5, 6, 8, 10, 11, 12, 14},\n",
       "             7: {2, 4, 5},\n",
       "             9: {2, 5, 8},\n",
       "             6: {3, 12, 13},\n",
       "             4: {7, 12, 13},\n",
       "             12: {4, 5, 6, 11, 13, 14},\n",
       "             10: {11, 13, 14}})"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
